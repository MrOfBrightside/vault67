#!/usr/bin/env bash
set -euo pipefail

# vault67 - CLI for multi-agent ticket refinement
# Usage: vault67 <command> [args]

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
TICKETS_DIR="$SCRIPT_DIR/tickets"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

error() {
    echo -e "${RED}Error: $*${NC}" >&2
    exit 1
}

success() {
    echo -e "${GREEN}✓ $*${NC}"
}

info() {
    echo -e "${BLUE}→ $*${NC}"
}

warn() {
    echo -e "${YELLOW}⚠ $*${NC}"
}

# Load Forgejo API configuration
load_config() {
    # Try loading from .vault67.conf file first
    local config_file="$SCRIPT_DIR/.vault67.conf"
    if [ -f "$config_file" ]; then
        source "$config_file"
    fi
    
    # Environment variables override config file
    FORGEJO_TOKEN="${FORGEJO_TOKEN:-}"
    FORGEJO_API="${FORGEJO_API:-https://git.logikfabriken.se/api/v1}"
    FORGEJO_REPO="${FORGEJO_REPO:-jesper/Vault67}"

    # Ollama (local LLM) configuration
    OLLAMA_URL="${OLLAMA_URL:-http://localhost:11434}"
    OLLAMA_MODEL="${OLLAMA_MODEL:-llama3.2:3b}"
    
    # For commands that need API access, validate token is set
    # (We'll check this in individual commands)
}

# Validate API configuration is present
require_api_config() {
    if [ -z "$FORGEJO_TOKEN" ]; then
        error "FORGEJO_TOKEN not set. Please set it in .vault67.conf or as an environment variable."
    fi
    if [ -z "$FORGEJO_API" ]; then
        error "FORGEJO_API not set."
    fi
    if [ -z "$FORGEJO_REPO" ]; then
        error "FORGEJO_REPO not set."
    fi
}



# ============================================================================
# Forgejo API Functions
# ============================================================================

# Make API request with proper auth and error handling
api_request() {
    local method="$1"
    local endpoint="$2"
    local data="${3:-}"
    
    local url="${FORGEJO_API}${endpoint}"
    local response_file=$(mktemp)
    local http_code
    
    if [ -n "$data" ]; then
        http_code=$(curl -s -w "%{http_code}" -X "$method" \
            -H "Authorization: token ${FORGEJO_TOKEN}" \
            -H "Content-Type: application/json" \
            -H "Accept: application/json" \
            -d "$data" \
            "$url" -o "$response_file")
    else
        http_code=$(curl -s -w "%{http_code}" -X "$method" \
            -H "Authorization: token ${FORGEJO_TOKEN}" \
            -H "Accept: application/json" \
            "$url" -o "$response_file")
    fi
    
    # Check HTTP status
    if [[ $http_code -ge 200 && $http_code -lt 300 ]]; then
        cat "$response_file"
        rm "$response_file"
        return 0
    else
        error "API request failed with status $http_code: $(cat "$response_file")"
        rm "$response_file"
        return 1
    fi
}

# Extract issue number from API response
extract_issue_number() {
    python3 -c "import sys, json; print(json.load(sys.stdin)['number'])"
}

# Extract issue body from API response
extract_issue_body() {
    python3 -c "import sys, json; print(json.load(sys.stdin)['body'])"
}

# Extract issue labels from API response (returns space-separated list)
extract_issue_labels() {
    python3 -c "import sys, json; print(' '.join([label['name'] for label in json.load(sys.stdin)['labels']]))"
}

# Create a new issue
# Args: title, body
api_create_issue() {
    local title="$1"
    local body="$2"
    
    local owner_repo="${FORGEJO_REPO}"
    local json_data=$(python3 -c "import json, sys; print(json.dumps({'title': sys.argv[1], 'body': sys.argv[2]}))" "$title" "$body")
    
    api_request "POST" "/repos/${owner_repo}/issues" "$json_data"
}

# Get an issue by number
# Args: issue_number
api_get_issue() {
    local issue_number="$1"
    local owner_repo="${FORGEJO_REPO}"
    
    api_request "GET" "/repos/${owner_repo}/issues/${issue_number}"
}

# Update an issue
# Args: issue_number, body (optional), title (optional)
api_update_issue() {
    local issue_number="$1"
    local new_body="${2:-}"
    local new_title="${3:-}"
    
    local owner_repo="${FORGEJO_REPO}"
    
    # Build JSON data dynamically
    local json_data="{}"
    if [ -n "$new_body" ]; then
        json_data=$(python3 -c "import json, sys; print(json.dumps({'body': sys.argv[1]}))" "$new_body")
    fi
    if [ -n "$new_title" ]; then
        if [ "$json_data" = "{}" ]; then
            json_data=$(python3 -c "import json, sys; print(json.dumps({'title': sys.argv[1]}))" "$new_title")
        else
            json_data=$(python3 -c "import json, sys; d=json.loads(sys.argv[1]); d['title']=sys.argv[2]; print(json.dumps(d))" "$json_data" "$new_title")
        fi
    fi
    
    api_request "PATCH" "/repos/${owner_repo}/issues/${issue_number}" "$json_data"
}

# Add label to an issue
# Args: issue_number, label_name
api_add_label() {
    local issue_number="$1"
    local label_name="$2"
    local owner_repo="${FORGEJO_REPO}"
    
    local json_data=$(python3 -c "import json, sys; print(json.dumps({'labels': [int(sys.argv[1])] if sys.argv[1].isdigit() else [sys.argv[1]]}))" "$label_name")
    
    api_request "POST" "/repos/${owner_repo}/issues/${issue_number}/labels" "$json_data"
}

# Replace all labels on an issue
# Args: issue_number, label_name (single label to set)
api_replace_labels() {
    local issue_number="$1"
    local label_name="$2"
    local owner_repo="${FORGEJO_REPO}"
    
    local json_data=$(python3 -c "import json, sys; print(json.dumps({'labels': [sys.argv[1]]}))" "$label_name")
    
    api_request "PUT" "/repos/${owner_repo}/issues/${issue_number}/labels" "$json_data"
}

# Remove label from an issue
# Args: issue_number, label_id
api_remove_label() {
    local issue_number="$1"
    local label_id="$2"
    local owner_repo="${FORGEJO_REPO}"
    
    api_request "DELETE" "/repos/${owner_repo}/issues/${issue_number}/labels/${label_id}"
}

# Add a comment to an issue
# Args: issue_number, comment_body
api_add_comment() {
    local issue_number="$1"
    local comment_body="$2"
    local owner_repo="${FORGEJO_REPO}"
    
    local json_data=$(python3 -c "import json, sys; print(json.dumps({'body': sys.argv[1]}))" "$comment_body")
    
    api_request "POST" "/repos/${owner_repo}/issues/${issue_number}/comments" "$json_data"
}

# Get all comments for an issue
# Args: issue_number
api_get_comments() {
    local issue_number="$1"
    local owner_repo="${FORGEJO_REPO}"
    
    api_request "GET" "/repos/${owner_repo}/issues/${issue_number}/comments"
}


# ============================================================================
# Dependency API Functions (Forgejo issue dependencies)
# ============================================================================

# Extract owner and repo from FORGEJO_REPO (e.g., "jesper/Vault67")
_dep_owner() { echo "${FORGEJO_REPO%%/*}"; }
_dep_repo()  { echo "${FORGEJO_REPO##*/}"; }

# Add a dependency: issue_number depends on depends_on_number
# Args: issue_number, depends_on_number
api_add_dependency() {
    local issue_number="$1"
    local depends_on_number="$2"
    local owner=$(_dep_owner)
    local repo=$(_dep_repo)

    local json_data=$(python3 -c "
import json, sys
print(json.dumps({'index': int(sys.argv[1]), 'owner': sys.argv[2], 'repo': sys.argv[3]}))
" "$depends_on_number" "$owner" "$repo")

    api_request "POST" "/repos/${FORGEJO_REPO}/issues/${issue_number}/dependencies" "$json_data"
}

# Get dependencies: what does this issue depend on?
# Args: issue_number
# Returns: JSON array of issues this depends on
api_get_dependencies() {
    local issue_number="$1"
    api_request "GET" "/repos/${FORGEJO_REPO}/issues/${issue_number}/dependencies"
}

# Get blocks: what does this issue block?
# Args: issue_number
# Returns: JSON array of issues this blocks
api_get_blocks() {
    local issue_number="$1"
    api_request "GET" "/repos/${FORGEJO_REPO}/issues/${issue_number}/blocks"
}

# Remove a dependency: issue_number no longer depends on depends_on_number
# Args: issue_number, depends_on_number
api_remove_dependency() {
    local issue_number="$1"
    local depends_on_number="$2"
    local owner=$(_dep_owner)
    local repo=$(_dep_repo)

    local json_data=$(python3 -c "
import json, sys
print(json.dumps({'index': int(sys.argv[1]), 'owner': sys.argv[2], 'repo': sys.argv[3]}))
" "$depends_on_number" "$owner" "$repo")

    api_request "DELETE" "/repos/${FORGEJO_REPO}/issues/${issue_number}/dependencies" "$json_data"
}

# Check if an issue has all dependencies resolved (all deps closed)
# Args: issue_number
# Returns: 0 if all deps resolved (or no deps), 1 if blocked
# Outputs blocking issue numbers to stdout
api_check_deps_resolved() {
    local issue_number="$1"

    local deps_response
    deps_response=$(api_get_dependencies "$issue_number" 2>/dev/null) || {
        # If API call fails, assume no deps (don't block)
        return 0
    }

    python3 -c "
import sys, json
try:
    deps = json.load(sys.stdin)
    if not deps:
        sys.exit(0)
    blockers = [d for d in deps if d.get('state') != 'closed']
    if blockers:
        for b in blockers:
            print(b['number'])
        sys.exit(1)
    sys.exit(0)
except SystemExit:
    raise
except Exception:
    sys.exit(0)
" <<< "$deps_response"
}


# Generate next ticket ID
generate_ticket_id() {
    local max_id=0
    if [ -d "$TICKETS_DIR" ]; then
        for dir in "$TICKETS_DIR"/TCK-*; do
            if [ -d "$dir" ]; then
                local id=$(basename "$dir" | sed 's/TCK-//')
                if [ "$id" -gt "$max_id" ]; then
                    max_id=$id
                fi
            fi
        done
    fi
    printf "TCK-%06d" $((max_id + 1))
}

# Create ticket command
cmd_create() {
    local title=""
    local repo=""
    local base_ref="main"

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            --title)
                title="$2"
                shift 2
                ;;
            --repo)
                repo="$2"
                shift 2
                ;;
            --base-ref)
                base_ref="$2"
                shift 2
                ;;
            *)
                error "Unknown option: $1"
                ;;
        esac
    done

    # Validate required arguments
    [ -z "$title" ] && error "--title is required"
    [ -z "$repo" ] && error "--repo is required"

    # Ensure API config is loaded
    require_api_config

    local timestamp=$(date -u +"%Y-%m-%d")

    info "Creating issue on Forgejo..."
    info "Title: $title"
    info "Repo path: $repo"
    info "Base ref: $base_ref"

    # Build issue body with spec template
    local issue_body="# Specification

**Metadata:**
- Repo: \`${repo}\`
- Base ref: \`${base_ref}\`
- Created: ${timestamp}

## Context
Why is this needed? What problem does it solve?

## Goal
What must be true after implementation?

## Scope
### In scope
-

### Out of scope
-

## Requirements (Raw, BA input)
-

## Acceptance Criteria (Gherkin)
\`\`\`gherkin
Feature: <feature name>

  Scenario: <scenario name>
    Given
    When
    Then
\`\`\`

## Architecture alignment
- Relevant modules:
- Constraints:
- Allowed paths:
- Forbidden paths:

## Security and compliance
- Data classification:
- AuthN/AuthZ:
- Logging/Audit:
- PII/Secrets:
- Security constraints:

## Test strategy
- Golden build command:
- Golden test command:
- Scenario to test mapping:
  - Scenario:
    - Test type (unit/integration/e2e/manual):
    - Suggested location (folder/file):

## Engineering principles and DoD additions
-

## Open questions
-

## Definition of Done
- PR created with changes scoped correctly
- Tests added/updated
- All required checks green
- Acceptance criteria satisfied
- Documentation updated (if needed)

## Definition of Ready
- [ ] Scope in/out defined
- [ ] Gherkin scenarios are present and testable
- [ ] Architecture alignment reviewed and constraints captured
- [ ] Security/compliance reviewed and constraints captured
- [ ] Test strategy defined for each scenario
- [ ] Repo golden commands known or explicitly blocked
- [ ] Allowed/forbidden paths set
- [ ] No blocking questions remain"

    # Create issue via API
    local api_response=$(api_create_issue "$title" "$issue_body")
    local issue_number=$(echo "$api_response" | extract_issue_number)
    local issue_url="https://git.logikfabriken.se/${FORGEJO_REPO}/issues/${issue_number}"

    success "Created issue #${issue_number}"

    # Add state:NEW label
    info "Adding state:NEW label..."
    api_replace_labels "$issue_number" "state:NEW" > /dev/null

    echo ""
    success "Issue created successfully!"
    success "Issue #${issue_number}: ${title}"
    success "URL: ${issue_url}"
    echo ""
    info "Next steps:"
    echo "  1. Edit the issue body to fill in requirements"
    echo "  2. Run: vault67 refine ${issue_number}"
}

# Get ticket state from ticket.md
get_ticket_state() {
    local ticket_file="$1"
    [ ! -f "$ticket_file" ] && error "Ticket file not found: $ticket_file"

    # Extract state from YAML frontmatter
    sed -n '/^---$/,/^---$/p' "$ticket_file" | grep '^state:' | awk '{print $2}'
}

# Update ticket state in ticket.md
update_ticket_state() {
    local ticket_file="$1"
    local new_state="$2"
    local timestamp=$(date -u +"%Y-%m-%d")

    [ ! -f "$ticket_file" ] && error "Ticket file not found: $ticket_file"

    # Update state and updated_at in YAML frontmatter
    sed -i.bak "s/^state: .*/state: $new_state/" "$ticket_file"
    sed -i.bak "s/^updated_at: .*/updated_at: $timestamp/" "$ticket_file"
    rm -f "$ticket_file.bak"
}

# Bump spec version in ticket.md
bump_spec_version() {
    local ticket_file="$1"
    [ ! -f "$ticket_file" ] && error "Ticket file not found: $ticket_file"

    # Extract current version
    local current_version=$(sed -n '/^---$/,/^---$/p' "$ticket_file" | grep '^spec_version:' | awk '{print $2}')
    local new_version=$((current_version + 1))

    # Update spec_version
    sed -i.bak "s/^spec_version: .*/spec_version: $new_version/" "$ticket_file"
    rm -f "$ticket_file.bak"

    echo "$new_version"
}

# Judge Agent - Evaluates Definition of Ready
judge_agent() {
    local spec_file="$1"
    local questions_file="$2"

    [ ! -f "$spec_file" ] && error "spec.md not found: $spec_file"

    info "Running Judge Agent (Gatekeeper)..."

    local all_criteria_met=true
    local blocking_questions_exist=false
    local dor_results=()

    # Extract content from spec.md
    local spec_content=$(cat "$spec_file")

    # Check 1: Scope in/out defined (reject placeholders)
    local has_scope=false
    local scope_in=$(echo "$spec_content" | sed -n '/### In scope/,/### /p' | sed '1d;$d')
    local scope_out=$(echo "$spec_content" | sed -n '/### Out of scope/,/## /p' | sed '1d;$d')
    if echo "$scope_in" | grep -q -e "^-[[:space:]]*[^[:space:]]" && \
       echo "$scope_out" | grep -q -e "^-[[:space:]]*[^[:space:]]"; then
        # Reject generic placeholders
        if echo "$scope_out" | grep -qiE "to be determined|TBD|TODO|not yet defined"; then
            has_scope=false
            all_criteria_met=false
            dor_results+=("[ ] Scope in/out defined (out of scope is placeholder)")
        else
            has_scope=true
            dor_results+=("[x] Scope in/out defined")
        fi
    else
        has_scope=false
        all_criteria_met=false
        dor_results+=("[ ] Scope in/out defined")
    fi

    # Check 2: Gherkin scenarios present, testable, and not generic placeholders
    local has_gherkin=false
    local gherkin_block=$(echo "$spec_content" | sed -n '/## Acceptance Criteria (Gherkin)/,/^## /p')
    if echo "$gherkin_block" | grep -q "Feature:\s\+\w" && \
       echo "$gherkin_block" | grep -q "Scenario:\s\+\w" && \
       echo "$gherkin_block" | grep -q "Given\s\+\w" && \
       echo "$gherkin_block" | grep -q "When\s\+\w" && \
       echo "$gherkin_block" | grep -q "Then\s\+\w"; then
        # Reject generic/placeholder Gherkin
        if echo "$gherkin_block" | grep -qiE "works as expected|the feature is used|the system is configured|expected behavior occurs|feature name|scenario name"; then
            has_gherkin=false
            all_criteria_met=false
            dor_results+=("[ ] Gherkin scenarios are present and testable (generic placeholders detected)")
        else
            has_gherkin=true
            dor_results+=("[x] Gherkin scenarios are present and testable")
        fi
    else
        has_gherkin=false
        all_criteria_met=false
        dor_results+=("[ ] Gherkin scenarios are present and testable")
    fi

    # Check 3: Architecture alignment reviewed and constraints captured
    local has_architecture=false
    if echo "$spec_content" | grep -A 10 "## Architecture alignment" | grep -q -e "- Detected stack:[[:space:]]*[^[:space:]]" || \
       echo "$spec_content" | grep -A 10 "## Architecture alignment" | grep -q -e "- Constraints:[[:space:]]*[^[:space:]]"; then
        has_architecture=true
        dor_results+=("[x] Architecture alignment reviewed and constraints captured")
    else
        has_architecture=false
        all_criteria_met=false
        dor_results+=("[ ] Architecture alignment reviewed and constraints captured")
    fi

    # Check 4: Security/compliance reviewed and constraints captured
    local has_security=false
    if echo "$spec_content" | grep -A 10 "## Security and compliance" | grep -q -e "- [[:alnum:]]*:[[:space:]]*[^[:space:]]" || \
       echo "$spec_content" | grep -A 10 "## Security and compliance" | grep -q -E "N/A|not applicable"; then
        has_security=true
        dor_results+=("[x] Security/compliance reviewed and constraints captured")
    else
        has_security=false
        all_criteria_met=false
        dor_results+=("[ ] Security/compliance reviewed and constraints captured")
    fi

    # Check 5: Test strategy defined with test layers
    local has_test_strategy=false
    if echo "$spec_content" | grep -q -e "- Golden build command:[[:space:]]*[^[:space:]]" && \
       echo "$spec_content" | grep -q -e "- Golden test command:[[:space:]]*[^[:space:]]" && \
       echo "$spec_content" | grep -qE "^- (Unit|Integration|E2E|e2e) tests:"; then
        has_test_strategy=true
        dor_results+=("[x] Test strategy defined with test layers")
    else
        has_test_strategy=false
        all_criteria_met=false
        dor_results+=("[ ] Test strategy defined with test layers")
    fi

    # Check 6: Repo golden commands known or explicitly blocked
    local has_golden_commands=false
    if echo "$spec_content" | grep -q -e "- Golden build command:[[:space:]]*[^[:space:]]" && \
       echo "$spec_content" | grep -q -e "- Golden test command:[[:space:]]*[^[:space:]]"; then
        has_golden_commands=true
        dor_results+=("[x] Repo golden commands known or explicitly blocked")
    else
        has_golden_commands=false
        all_criteria_met=false
        dor_results+=("[ ] Repo golden commands known or explicitly blocked")
    fi

    # Check 7: Allowed/forbidden paths set
    local has_paths=false
    if echo "$spec_content" | grep -A 10 "## Architecture alignment" | grep -q -e "- Allowed paths:[[:space:]]*[^[:space:]]" || \
       echo "$spec_content" | grep -A 10 "## Architecture alignment" | grep -q -e "- Forbidden paths:[[:space:]]*[^[:space:]]"; then
        has_paths=true
        dor_results+=("[x] Allowed/forbidden paths set")
    else
        has_paths=false
        all_criteria_met=false
        dor_results+=("[ ] Allowed/forbidden paths set")
    fi

    # Check 8: No blocking questions remain
    local has_no_blocking_questions=true
    if [ -f "$questions_file" ]; then
        # Extract blocking questions section
        local questions_section=$(sed -n '/## Blocking questions/,/## Notes/p' "$questions_file")

        # Check if there are actual questions with content (not just template placeholders)
        # Look for lines like "1) Question: something" or "Question: something" where something is not empty
        if echo "$questions_section" | grep -E "^\s*[0-9]+\)\s+Question:\s*.+$" > /dev/null; then
            # There are questions with content - check if they have answers
            if echo "$questions_section" | grep -E "Answer:\s*$" > /dev/null; then
                # Questions exist but some answers are empty
                has_no_blocking_questions=false
                blocking_questions_exist=true
                all_criteria_met=false
                dor_results+=("[ ] No blocking questions remain")
            else
                # All questions have answers
                dor_results+=("[x] No blocking questions remain")
            fi
        else
            # No real questions (just template), consider it as no blocking questions
            dor_results+=("[x] No blocking questions remain")
        fi
    else
        dor_results+=("[x] No blocking questions remain")
    fi

    # Update Definition of Ready checklist in spec.md
    local temp_file=$(mktemp)
    local in_dor_section=false
    local dor_index=0

    while IFS= read -r line; do
        if [[ "$line" == "## Definition of Ready" ]]; then
            in_dor_section=true
            echo "$line" >> "$temp_file"
        elif [[ "$in_dor_section" == true && "$line" =~ ^\-[[:space:]]\[[[:space:]]\] ]]; then
            if [ $dor_index -lt ${#dor_results[@]} ]; then
                echo "- ${dor_results[$dor_index]}" >> "$temp_file"
                dor_index=$((dor_index + 1))
            else
                echo "$line" >> "$temp_file"
            fi
        else
            if [[ "$in_dor_section" == true && ! "$line" =~ ^\-[[:space:]]\[ ]]; then
                in_dor_section=false
            fi
            echo "$line" >> "$temp_file"
        fi
    done < "$spec_file"

    mv "$temp_file" "$spec_file"

    # Print evaluation results
    echo ""
    info "Definition of Ready Evaluation:"
    for result in "${dor_results[@]}"; do
        if [[ "$result" == "[x]"* ]]; then
            echo -e "  ${GREEN}✓${NC} ${result#[x] }"
        else
            echo -e "  ${RED}✗${NC} ${result#[ ] }"
        fi
    done
    echo ""

    # Return status
    if [ "$blocking_questions_exist" = true ]; then
        warn "Blocking questions exist - ticket needs information"
        return 2  # NEEDS_INFO
    elif [ "$all_criteria_met" = true ]; then
        success "All Definition of Ready criteria met"
        return 0  # READY_TO_IMPLEMENT
    else
        warn "Not all criteria met - ticket not ready"
        return 1  # Not ready
    fi
}

# Generate promptpack.md from spec.md
generate_promptpack() {
    local spec_file="$1"
    local ticket_file="$2"
    local promptpack_file="$3"

    [ ! -f "$spec_file" ] && error "spec.md not found: $spec_file"
    [ ! -f "$ticket_file" ] && error "ticket.md not found: $ticket_file"

    info "Generating promptpack.md..."

    # Extract values from ticket.md
    local base_ref=$(sed -n '/^---$/,/^---$/p' "$ticket_file" | grep '^base_ref:' | awk '{print $2}')

    # Extract sections from spec.md
    local context=$(sed -n '/^## Context$/,/^##/p' "$spec_file" | sed '1d;$d')
    local goal=$(sed -n '/^## Goal$/,/^##/p' "$spec_file" | sed '1d;$d')
    local scope_in=$(sed -n '/^### In scope$/,/^###/p' "$spec_file" | sed '1d;$d')
    local scope_out=$(sed -n '/^### Out of scope$/,/^##/p' "$spec_file" | sed '1d;$d')
    local gherkin=$(sed -n '/^## Acceptance Criteria (Gherkin)$/,/^##/p' "$spec_file" | sed '1d;$d')
    local architecture=$(sed -n '/^## Architecture alignment$/,/^##/p' "$spec_file" | sed '1d;$d')
    local security=$(sed -n '/^## Security and compliance$/,/^##/p' "$spec_file" | sed '1d;$d')
    local test_strategy=$(sed -n '/^## Test strategy$/,/^##/p' "$spec_file" | sed '1d;$d')

    # Extract golden commands
    local golden_build=$(echo "$test_strategy" | grep "- Golden build command:" | sed 's/- Golden build command:\s*//')
    local golden_test=$(echo "$test_strategy" | grep "- Golden test command:" | sed 's/- Golden test command:\s*//')

    # Write promptpack.md
    cat > "$promptpack_file" <<EOF
# Implementation Prompt Pack (for Gas Town)

## Objective
$goal

## Context
$context

## Scope
### In scope
$scope_in

### Out of scope
$scope_out

## Acceptance criteria (Gherkin, must satisfy)
$gherkin

## Constraints and guardrails
### Architecture alignment
$architecture

### Security/compliance constraints
$security

## Repo instructions
### Base ref
- $base_ref

### How to build (golden command)
$golden_build

### How to test (golden command)
$golden_test

## Test strategy
$test_strategy

## Verification checklist
- [ ] All tests pass using golden commands
- [ ] Acceptance criteria satisfied
- [ ] Quality gates pass (lint, typecheck — see test strategy)
- [ ] No out-of-scope changes
- [ ] Required docs updated (if applicable)

## Expected output
- Create a PR against base ref
- Include a brief PR description mapping changes to scenarios
- Include test results summary
EOF

    success "Generated promptpack.md"
}

# Architecture Compliance Agent - Analyzes architecture alignment
architecture_compliance_agent() {
    local ticket_dir="$1"
    local spec_file="$ticket_dir/spec.md"
    local repo_context_file="$ticket_dir/repo_context.md"
    local default_rules="$SCRIPT_DIR/agents/architecture.md"

    [ ! -f "$spec_file" ] && error "spec.md not found: $spec_file"
    [ ! -f "$repo_context_file" ] && error "repo_context.md not found: $repo_context_file"

    info "Analyzing architecture alignment..."

    # Check if architecture section already has substantive content
    local existing_arch=$(sed -n '/^## Architecture alignment/,/^## /p' "$spec_file" | sed '1d;$d')
    if echo "$existing_arch" | grep -q -e "- Detected stack:[[:space:]]*[^[:space:]]" || \
       echo "$existing_arch" | grep -q -e "- Relevant modules:[[:space:]]*[^[:space:]]"; then
        success "Architecture alignment section already filled in - preserving"
        return 0
    fi

    # Helper to read a section from a markdown file
    _read_md_section() {
        local section="$1" file="$2"
        awk "/^## ${section}\$/{found=1;next} /^## /{if(found)exit} found && !/^#/ && !/^[[:space:]]*\$/{print}" "$file" 2>/dev/null
    }

    # Get repo path
    local repo_path=$(grep "^- Path/URL:" "$repo_context_file" 2>/dev/null | sed 's/^- Path\/URL:[[:space:]]*//')

    # --- Step 1: Detect language/stack from ## Language detection ---
    local detected_stack=""
    local _marker_files=""  # collect matched markers for root file reporting
    if [ -n "$repo_path" ] && [ -d "$repo_path" ] 2>/dev/null; then
        info "Inspecting repository at ${repo_path}..."

        if [ -f "$default_rules" ]; then
            local in_lang=false
            while IFS= read -r line; do
                case "$line" in
                    "## Language detection"*) in_lang=true; continue ;;
                    "## "*) [ "$in_lang" = true ] && break; continue ;;
                    "#"*|"") continue ;;
                esac
                if [ "$in_lang" = true ] && [[ "$line" == -\ * ]]; then
                    local marker=$(echo "$line" | sed 's/^- //' | cut -d: -f1 | tr -d ' ')
                    local rest=$(echo "$line" | cut -d: -f2-)
                    local lang=$(echo "$rest" | cut -d'|' -f1 | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
                    # Handle glob patterns (e.g. *.csproj)
                    local _matched=false
                    if [[ "$marker" == *"*"* ]]; then
                        ls "$repo_path"/$marker 1>/dev/null 2>&1 && _matched=true
                    elif [ -f "$repo_path/$marker" ]; then
                        _matched=true
                    fi
                    if [ "$_matched" = true ]; then
                        _marker_files="${_marker_files:+$_marker_files, }${marker}"
                        # Deduplicate: only add lang if not already in stack
                        if ! echo "$detected_stack" | grep -q "$lang"; then
                            detected_stack="${detected_stack:+$detected_stack, }${lang}"
                        fi
                    fi
                fi
            done < "$default_rules"
        fi
    fi
    [ -z "$detected_stack" ] && detected_stack="Not detected (repo not accessible)"

    # --- Step 2: Scan directory structure using ## Skip dirs ---
    local repo_structure=""
    if [ -n "$repo_path" ] && [ -d "$repo_path" ] 2>/dev/null; then
        # Build skip list from rules file
        local _skip_dirs=""
        if [ -f "$default_rules" ]; then
            _skip_dirs=$(_read_md_section "Skip dirs" "$default_rules" | tr ',' '\n' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//' | sed 's/^/|/' | tr -d '\n')
            _skip_dirs="${_skip_dirs}|"
        fi

        local _dir_list=""
        for d in "$repo_path"/*/; do
            [ -d "$d" ] || continue
            local dirname=$(basename "$d")
            # Skip hidden dirs and dirs in skip list
            [[ "$dirname" == .* ]] && continue
            [ -n "$_skip_dirs" ] && echo "$_skip_dirs" | grep -q "|${dirname}|" && continue
            local count=$(ls -1 "$d" 2>/dev/null | wc -l | tr -d ' ')
            _dir_list="${_dir_list:+$_dir_list, }${dirname}/ (${count} items)"
        done
        repo_structure="$_dir_list"

        # Report matched root marker files
        [ -n "$_marker_files" ] && repo_structure="${repo_structure:+$repo_structure | Root files: }${_marker_files}"
    fi

    # --- Step 3: Map Gherkin keywords to actual directories ---
    local relevant_dirs=""
    local gherkin_section=$(sed -n '/## Acceptance Criteria (Gherkin)/,/^## /p' "$spec_file" | sed '$d')
    local scenarios=$(echo "$gherkin_section" | grep -E "Scenario:|Given|When|Then" 2>/dev/null)

    if [ -n "$repo_path" ] && [ -d "$repo_path" ] 2>/dev/null && [ -n "$scenarios" ]; then
        # Build stop word regex from ## Stop words
        local _stop_re="^(given|when|then|that|this|from|with)$"
        if [ -f "$default_rules" ]; then
            local _stop_words=$(_read_md_section "Stop words" "$default_rules" | tr ',' '\n' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//' | paste -sd'|' -)
            [ -n "$_stop_words" ] && _stop_re="^(${_stop_words})$"
        fi

        # Build find exclusion args from ## Skip dirs
        local _find_excludes=""
        if [ -f "$default_rules" ]; then
            _find_excludes=$(_read_md_section "Skip dirs" "$default_rules" | tr ',' '\n' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//' | while read -r skip; do
                [ -n "$skip" ] && echo "-not -path */${skip}/*"
            done | tr '\n' ' ')
        fi

        # Extract nouns/verbs from scenarios for matching
        local scenario_words=$(echo "$scenarios" | tr '[:upper:]' '[:lower:]' | \
            sed 's/[^a-z ]/ /g' | tr ' ' '\n' | sort -u | \
            awk 'length >= 4' | grep -vE "$_stop_re")

        # Find directories containing files whose names match scenario keywords
        for word in $scenario_words; do
            local matches=$(eval "find \"$repo_path\" -maxdepth 3 -type f \
                \\( -name \"*${word}*\" -o -name \"*$(echo "$word" | sed 's/s$//')*\" \\) \
                $_find_excludes" 2>/dev/null | head -5)
            if [ -n "$matches" ]; then
                local dirs=$(echo "$matches" | while read -r f; do
                    dirname "$f" | sed "s|^${repo_path}/||"
                done | sort -u)
                for d in $dirs; do
                    if ! echo "$relevant_dirs" | grep -q "$d"; then
                        relevant_dirs="${relevant_dirs:+$relevant_dirs, }${d}"
                    fi
                done
            fi
        done
    fi

    # --- Step 4: Load per-project config (ARCHITECTURE.md in repo root) ---
    local project_constraints=""
    local project_forbidden=""
    local project_notes=""
    local repo_config=""

    if [ -n "$repo_path" ] && [ -d "$repo_path" ] 2>/dev/null; then
        for cfg in "$repo_path/ARCHITECTURE.md" "$repo_path/.vault67/architecture.md" "$repo_path/.github/architecture.md"; do
            if [ -f "$cfg" ]; then
                repo_config="$cfg"
                break
            fi
        done
    fi

    if [ -n "$repo_config" ]; then
        info "Found project config: ${repo_config}"
        project_constraints=$(_read_md_section "Constraints" "$repo_config")
        local proj_forbidden=$(_read_md_section "Forbidden paths" "$repo_config")
        [ -n "$proj_forbidden" ] && project_forbidden="$proj_forbidden"
        project_notes=$(_read_md_section "Notes" "$repo_config")
    fi

    # Fall back to default config for forbidden paths
    if [ -z "$project_forbidden" ] && [ -f "$default_rules" ]; then
        project_forbidden=$(_read_md_section "Forbidden paths" "$default_rules")
    fi

    # --- Step 5: Build output ---
    local temp_arch=$(mktemp)
    {
        echo "## Architecture alignment"
        echo "- Detected stack: ${detected_stack}"
        if [ -n "$repo_structure" ]; then
            echo "- Repo structure: ${repo_structure}"
        fi
        if [ -n "$relevant_dirs" ]; then
            echo "- Relevant paths for this feature: ${relevant_dirs}"
        fi
        if [ -n "$project_constraints" ]; then
            echo "- Constraints: ${project_constraints}"
        fi
        if [ -n "$project_forbidden" ]; then
            echo "- Forbidden paths: ${project_forbidden}"
        fi
        if [ -n "$project_notes" ]; then
            echo ""
            echo "### Project notes"
            echo "$project_notes"
        fi
    } > "$temp_arch"

    awk '
        /^## Architecture alignment/ {
            system("cat '"$temp_arch"'")
            in_section = 1
            next
        }
        /^## / && in_section {
            in_section = 0
        }
        !in_section { print }
    ' "$spec_file" > "$spec_file.tmp" && mv "$spec_file.tmp" "$spec_file"

    rm -f "$temp_arch"
    if [ -n "$repo_config" ]; then
        success "Architecture section updated (repo inspected + project config)"
    elif [ -n "$repo_structure" ]; then
        success "Architecture section updated (repo inspected)"
    else
        success "Architecture section updated (repo not accessible — defaults only)"
    fi
    return 0
}

# Security & Compliance Agent - Analyzes security requirements
security_compliance_agent() {
    local ticket_dir="$1"
    local spec_file="$ticket_dir/spec.md"
    local repo_context_file="$ticket_dir/repo_context.md"
    local rules_file="$SCRIPT_DIR/agents/security.md"

    [ ! -f "$spec_file" ] && error "spec.md not found: $spec_file"
    [ ! -f "$repo_context_file" ] && error "repo_context.md not found: $repo_context_file"

    info "Analyzing security and compliance requirements..."

    # Check if security section already has substantive content
    local existing_sec=$(sed -n '/^## Security and compliance/,/^## /p' "$spec_file" | sed '1d;$d')
    if echo "$existing_sec" | grep -q -e "- Data classification:[[:space:]]*[^[:space:]]"; then
        success "Security and compliance section already filled in - preserving"
        return 0
    fi

    local gherkin_section=$(sed -n '/## Acceptance Criteria (Gherkin)/,/^## /p' "$spec_file" | sed '$d')
    if ! echo "$gherkin_section" | grep -q "Feature:"; then
        warn "No Gherkin scenarios found - skipping security analysis"
        return 1
    fi

    info "Analyzing scenarios for security implications..."
    local scenarios=$(echo "$gherkin_section" | grep -E "Scenario:|Given|When|Then")

    # Helper: first-match keyword rule from a section in the rules file
    _match_rule() {
        local section="$1"
        local default_section="$2"
        local field_num="${3:-1}"  # which field after the colon (1-based, pipe-separated)

        if [ ! -f "$rules_file" ]; then echo ""; return; fi

        local in_section=false
        while IFS= read -r line; do
            case "$line" in
                "## ${section}"*) in_section=true; continue ;;
                "## "*) [ "$in_section" = true ] && break; continue ;;
                "#"*|"") continue ;;
            esac
            if [ "$in_section" = true ] && [[ "$line" == -\ * ]]; then
                local keywords=$(echo "$line" | sed 's/^- //' | cut -d: -f1 | tr ',' '|' | tr -d ' ' | tr '_' '.')
                local value=$(echo "$line" | cut -d: -f2- | sed 's/^[[:space:]]*//')
                if [ "$field_num" -gt 1 ]; then
                    value=$(echo "$value" | cut -d'|' -f"$field_num" | sed 's/^[[:space:]]*//')
                else
                    value=$(echo "$value" | cut -d'|' -f1 | sed 's/[[:space:]]*$//')
                fi
                if echo "$scenarios" | grep -qiE "$keywords"; then
                    echo "$value"
                    return
                fi
            fi
        done < "$rules_file"

        # Return default
        if [ -n "$default_section" ] && [ -f "$rules_file" ]; then
            awk "/^## ${default_section}\$/{found=1;next} /^## /{if(found)exit} found&&NF{printf \"%s\",\$0}" "$rules_file"
        fi
    }

    # Collect all matching additional constraints
    _collect_constraints() {
        if [ ! -f "$rules_file" ]; then echo ""; return; fi
        local result=""
        local in_section=false
        while IFS= read -r line; do
            case "$line" in
                "## Additional constraint rules"*) in_section=true; continue ;;
                "## "*) [ "$in_section" = true ] && break; continue ;;
                "#"*|"") continue ;;
            esac
            if [ "$in_section" = true ] && [[ "$line" == -\ * ]]; then
                local keywords=$(echo "$line" | sed 's/^- //' | cut -d: -f1 | tr ',' '|' | tr -d ' ' | tr '_' '.')
                local value=$(echo "$line" | cut -d: -f2- | sed 's/^[[:space:]]*//')
                if echo "$scenarios" | grep -qiE "$keywords"; then
                    result="${result}${result:+; }${value}"
                fi
            fi
        done < "$rules_file"
        echo "$result"
    }

    local data_classification=$(_match_rule "Data classification rules" "Default classification" 1)
    local pii_secrets=$(_match_rule "Data classification rules" "Default PII" 2)
    local authn_authz=$(_match_rule "AuthN/AuthZ rules" "Default AuthN/AuthZ" 1)
    local logging_audit=$(_match_rule "Logging rules" "Default logging" 1)

    local base_constraints=""
    [ -f "$rules_file" ] && base_constraints=$(awk '/^## Base security constraints$/{found=1;next} /^## /{if(found)exit} found&&NF{printf "%s",$0}' "$rules_file")
    local extra=$(_collect_constraints)
    local security_constraints="${base_constraints:-Follow secure coding practices}${extra:+; $extra}"

    # Defaults if rules file missing
    [ -z "$data_classification" ] && data_classification="PUBLIC"
    [ -z "$pii_secrets" ] && pii_secrets="No PII or secrets identified"
    [ -z "$authn_authz" ] && authn_authz="No authentication required (public access)"
    [ -z "$logging_audit" ] && logging_audit="Standard application logging"

    # Update spec.md
    local temp_section=$(mktemp)
    cat > "$temp_section" <<EOF
- Data classification: $data_classification
- AuthN/AuthZ: $authn_authz
- Logging/Audit: $logging_audit
- PII/Secrets: $pii_secrets
- Security constraints: $security_constraints
EOF

    awk '
        /^## Security and compliance/ {
            print
            print ""
            system("cat '"$temp_section"'")
            in_section = 1
            next
        }
        /^## / && in_section {
            in_section = 0
        }
        !in_section { print }
    ' "$spec_file" > "$spec_file.tmp" && mv "$spec_file.tmp" "$spec_file"

    rm -f "$temp_section"
    success "Security and compliance section updated"
    return 0
}

# ============================================================================
# Ollama (Local LLM) Integration
# ============================================================================

# Call Ollama API and return the response text
# Usage: ollama_generate "prompt text"
# Returns: response text on stdout, non-zero exit on failure
ollama_generate() {
    local prompt="$1"
    local response_file=$(mktemp)

    # Build JSON payload using python3 for safe escaping
    local json_payload
    json_payload=$(python3 -c "
import json, sys
print(json.dumps({
    'model': sys.argv[1],
    'prompt': sys.argv[2],
    'stream': False
}))
" "$OLLAMA_MODEL" "$prompt" 2>/dev/null)

    if [ -z "$json_payload" ]; then
        rm -f "$response_file"
        return 1
    fi

    local http_code
    http_code=$(curl -s -w "%{http_code}" --connect-timeout 5 --max-time 120 \
        -X POST "${OLLAMA_URL}/api/generate" \
        -H "Content-Type: application/json" \
        -d "$json_payload" \
        -o "$response_file" 2>/dev/null) || {
        rm -f "$response_file"
        return 1
    }

    if [[ "$http_code" != "200" ]]; then
        # Check for model not found — auto-pull
        if grep -q "model.*not found\|no such model" "$response_file" 2>/dev/null; then
            warn "Model '${OLLAMA_MODEL}' not found — pulling..."
            if /opt/homebrew/bin/ollama pull "$OLLAMA_MODEL" 2>&1 | tail -1; then
                success "Model '${OLLAMA_MODEL}' pulled successfully"
                rm -f "$response_file"
                # Retry the request
                http_code=$(curl -s -w "%{http_code}" --connect-timeout 5 --max-time 120 \
                    -X POST "${OLLAMA_URL}/api/generate" \
                    -H "Content-Type: application/json" \
                    -d "$json_payload" \
                    -o "$response_file" 2>/dev/null) || {
                    rm -f "$response_file"
                    return 1
                }
                if [[ "$http_code" != "200" ]]; then
                    rm -f "$response_file"
                    return 1
                fi
            else
                rm -f "$response_file"
                return 1
            fi
        else
            rm -f "$response_file"
            return 1
        fi
    fi

    # Extract .response from JSON
    local response_text
    response_text=$(python3 -c "
import json, sys
data = json.load(open(sys.argv[1]))
print(data.get('response', ''))
" "$response_file" 2>/dev/null)

    rm -f "$response_file"

    if [ -z "$response_text" ]; then
        return 1
    fi

    echo "$response_text"
    return 0
}

# BA Translator Agent - Uses local LLM to generate concrete Gherkin from spec
ba_translator_agent() {
    local ticket_dir="$1"
    local spec_file="$ticket_dir/spec.md"

    [ ! -f "$spec_file" ] && error "spec.md not found: $spec_file"

    info "Running BA Translator Agent (LLM-powered)..."

    # Check if Gherkin already has substantive content (not placeholder)
    local gherkin_block=$(sed -n '/## Acceptance Criteria (Gherkin)/,/^## /p' "$spec_file" | sed '1d;$d')
    if echo "$gherkin_block" | grep -q "Scenario:" && \
       ! echo "$gherkin_block" | grep -qiE "works as specified|works as expected|the feature is used|the system is configured|expected behavior occurs|feature name|scenario name|TODO.*needs clarification"; then
        success "Gherkin scenarios already contain substantive content - preserving"
        return 0
    fi

    # Extract key sections from spec.md for the prompt
    local context=$(sed -n '/^## Context$/,/^## /p' "$spec_file" | sed '1d;$d')
    local goal=$(sed -n '/^## Goal$/,/^## /p' "$spec_file" | sed '1d;$d')
    local requirements=$(sed -n '/^## Requirements/,/^## /p' "$spec_file" | sed '1d;$d')
    local scope_in=$(sed -n '/^### In scope$/,/^###\|^## /p' "$spec_file" | sed '1d;$d')
    local scope_out=$(sed -n '/^### Out of scope$/,/^## /p' "$spec_file" | sed '1d;$d')

    # Check if there's actually substance to translate — skip if all fields are
    # empty or only contain placeholder text (dashes, question prompts)
    local _has_substance=false
    for _field in "$context" "$goal" "$requirements" "$scope_in"; do
        # Strip placeholder lines: lone dashes, "Why is this needed?", "What must be true?", etc.
        local _clean=$(echo "$_field" | grep -vE '^[[:space:]]*-?[[:space:]]*$|^[[:space:]]*$|^Why is|^What must|^What problem' | head -1)
        if [ -n "$_clean" ]; then
            _has_substance=true
            break
        fi
    done
    if [ "$_has_substance" = false ]; then
        warn "No context, goal, or requirements filled in — skipping Gherkin generation"
        warn "Fill in the spec sections first, then re-run refine"
        return 1
    fi

    # Extract the feature title from existing Gherkin (if any)
    local feature_title=$(echo "$gherkin_block" | grep "Feature:" | head -1 | sed 's/.*Feature:[[:space:]]*//')
    # Fall back to issue title from spec metadata, then goal
    if [ -z "$feature_title" ]; then
        feature_title=$(grep -m1 "^# " "$spec_file" 2>/dev/null | sed 's/^# //')
    fi
    [ -z "$feature_title" ] && feature_title=$(echo "$goal" | head -1)

    # Build the LLM prompt
    local prompt="You are a Business Analyst writing Gherkin acceptance criteria for a software feature.

CONTEXT:
${context}

GOAL:
${goal}

REQUIREMENTS:
${requirements}

IN SCOPE:
${scope_in}

OUT OF SCOPE:
${scope_out}

INSTRUCTIONS:
Write concrete, testable Gherkin scenarios for this feature. Follow these rules strictly:
1. Use real, specific values - NEVER use placeholders like \"the feature works as specified\" or \"expected behavior occurs\"
2. Each Scenario must have concrete Given/When/Then steps with specific data
3. Cover the happy path and at least one error/edge case
4. Keep scenarios focused - one behavior per scenario
5. Use the exact feature title: ${feature_title}

OUTPUT FORMAT:
Return ONLY a valid Gherkin feature block. No markdown fences, no explanations before or after. Start directly with 'Feature:' and end after the last scenario. Example format:

Feature: User Login
  Scenario: Successful login with valid credentials
    Given a registered user with email \"alice@example.com\" and password \"secret123\"
    When the user submits the login form with email \"alice@example.com\" and password \"secret123\"
    Then the user is redirected to the dashboard
    And a session token is issued

  Scenario: Login fails with wrong password
    Given a registered user with email \"alice@example.com\"
    When the user submits the login form with email \"alice@example.com\" and password \"wrong\"
    Then an error message \"Invalid credentials\" is displayed
    And no session token is issued

If you are unsure about specific details, make reasonable assumptions based on the context and requirements. Do NOT leave anything as TODO or TBD."

    # Call Ollama
    local llm_response
    llm_response=$(ollama_generate "$prompt" 2>/dev/null) || {
        warn "Ollama unavailable (is 'ollama serve' running at ${OLLAMA_URL}?) — keeping placeholder Gherkin"
        return 1
    }

    if [ -z "$llm_response" ]; then
        warn "Empty response from LLM — keeping placeholder Gherkin"
        return 1
    fi

    # Extract just the Gherkin block from the response
    # The LLM might wrap it in markdown fences or add commentary
    local clean_gherkin
    clean_gherkin=$(echo "$llm_response" | python3 -c "
import sys
text = sys.stdin.read()

# Strip markdown fences if present
import re
fenced = re.search(r'\`\`\`(?:gherkin)?\s*\n(.*?)\`\`\`', text, re.DOTALL)
if fenced:
    text = fenced.group(1)

# Find the Feature: block
lines = text.split('\n')
output = []
in_feature = False
for line in lines:
    stripped = line.strip()
    if stripped.startswith('Feature:'):
        in_feature = True
    if in_feature:
        output.append(line)

if output:
    # Trim trailing blank lines
    while output and not output[-1].strip():
        output.pop()
    print('\n'.join(output))
else:
    # Fallback: print everything that looks like Gherkin
    for line in lines:
        s = line.strip()
        if s and any(s.startswith(kw) for kw in ['Feature:', 'Scenario:', 'Given ', 'When ', 'Then ', 'And ', 'But ']):
            print(line)
" 2>/dev/null)

    # Validate we got something useful
    if [ -z "$clean_gherkin" ] || ! echo "$clean_gherkin" | grep -q "Feature:"; then
        warn "LLM response did not contain valid Gherkin — keeping placeholder"
        return 1
    fi

    if ! echo "$clean_gherkin" | grep -q "Scenario:"; then
        warn "LLM response missing Scenario blocks — keeping placeholder"
        return 1
    fi

    # Final safety check: reject if LLM produced placeholder-like content
    if echo "$clean_gherkin" | grep -qiE "works as specified|works as expected|the feature is used|expected behavior occurs"; then
        warn "LLM produced placeholder-like Gherkin — keeping existing"
        return 1
    fi

    # Replace the Acceptance Criteria section in spec.md
    local temp_gherkin=$(mktemp)
    cat > "$temp_gherkin" <<GEOF
## Acceptance Criteria (Gherkin)
\`\`\`gherkin
${clean_gherkin}
\`\`\`
GEOF

    awk '
        /^## Acceptance Criteria \(Gherkin\)/ {
            system("cat '"$temp_gherkin"'")
            in_section = 1
            next
        }
        /^## / && in_section {
            in_section = 0
        }
        !in_section { print }
    ' "$spec_file" > "$spec_file.tmp" && mv "$spec_file.tmp" "$spec_file"

    rm -f "$temp_gherkin"
    success "Gherkin scenarios generated by LLM (${OLLAMA_MODEL})"
    return 0
}

# Scan repo and update repo_context.md
scan_repo() {
    local ticket_dir="$1"
    local ticket_file="$ticket_dir/ticket.md"
    local repo_context_file="$ticket_dir/repo_context.md"

    [ ! -f "$ticket_file" ] && error "ticket.md not found: $ticket_file"

    info "Scanning repository..."

    # Extract repo path from ticket.md
    local repo_path=$(sed -n '/^---$/,/^---$/p' "$ticket_file" | grep '^repo:' | sed 's/^repo:[[:space:]]*//')
    local base_ref=$(sed -n '/^---$/,/^---$/p' "$ticket_file" | grep '^base_ref:' | sed 's/^base_ref:[[:space:]]*//')

    [ -z "$repo_path" ] && error "No repo path found in ticket.md"

    # Resolve absolute path if it's relative
    if [[ ! "$repo_path" =~ ^/ ]]; then
        repo_path="$SCRIPT_DIR/$repo_path"
    fi

    [ ! -d "$repo_path" ] && error "Repository not found: $repo_path"

    info "Repository: $repo_path"
    info "Base ref: $base_ref"

    # Detect language/runtime
    local language=""
    local build_cmd=""
    local test_cmd=""

    if [ -f "$repo_path/package.json" ]; then
        language="JavaScript/TypeScript (Node.js)"
        build_cmd="npm run build"
        test_cmd="npm test"
    elif [ -f "$repo_path/go.mod" ]; then
        language="Go"
        build_cmd="go build ./..."
        test_cmd="go test ./..."
    elif [ -f "$repo_path/pom.xml" ]; then
        language="Java (Maven)"
        build_cmd="mvn compile"
        test_cmd="mvn test"
    elif [ -f "$repo_path/build.gradle" ] || [ -f "$repo_path/build.gradle.kts" ]; then
        language="Java/Kotlin (Gradle)"
        build_cmd="./gradlew build"
        test_cmd="./gradlew test"
    elif [ -f "$repo_path/Cargo.toml" ]; then
        language="Rust"
        build_cmd="cargo build"
        test_cmd="cargo test"
    elif [ -f "$repo_path/requirements.txt" ] || [ -f "$repo_path/setup.py" ] || [ -f "$repo_path/pyproject.toml" ]; then
        language="Python"
        build_cmd="python -m build (or N/A)"
        test_cmd="pytest"
    elif [ -f "$repo_path/Makefile" ]; then
        language="C/C++ or Make-based"
        build_cmd="make"
        test_cmd="make test"
    else
        language="Unknown (manual detection needed)"
        build_cmd="UNKNOWN - needs manual specification"
        test_cmd="UNKNOWN - needs manual specification"
    fi

    # Get directory structure (top-level folders)
    local main_components=""
    if [ -d "$repo_path" ]; then
        main_components=$(cd "$repo_path" && find . -maxdepth 2 -type d ! -path '*/\.*' ! -path '.' ! -path './node_modules*' ! -path './target*' ! -path './dist*' ! -path './build*' 2>/dev/null | head -20 | sort | sed 's|^\./||' | paste -sd ", " -)
    fi

    # Detect CI/CD
    local pipeline_files=""
    if [ -d "$repo_path/.github/workflows" ]; then
        pipeline_files=".github/workflows/*"
    elif [ -f "$repo_path/.gitlab-ci.yml" ]; then
        pipeline_files=".gitlab-ci.yml"
    elif [ -f "$repo_path/Jenkinsfile" ]; then
        pipeline_files="Jenkinsfile"
    elif [ -f "$repo_path/.circleci/config.yml" ]; then
        pipeline_files=".circleci/config.yml"
    fi

    # Update repo_context.md
    cat > "$repo_context_file" <<EOF
# Repo Context

## Repo
- Path/URL: $repo_path
- Base ref: $base_ref
- Language/runtime: $language
- Main components/modules: $main_components
- Architecture docs: (needs manual review)
- Coding conventions: (needs manual review)

## How to build (golden command)
- Command(s): $build_cmd
- Notes: Auto-detected, verify correctness

## How to test (golden command)
- Command(s): $test_cmd
- Test types present (unit/integration/e2e): needs scan
- Notes: Auto-detected, verify correctness

## CI/CD signals
- Pipeline file(s): $pipeline_files
- Quality gates (lint, typecheck, etc): needs review

## Relevant code areas
- Likely folders/modules: $main_components
- Key files (if known): (needs analysis based on requirements)

## Snippets (short)
> Keep snippets short. Prefer paths and small excerpts.
- Path: (to be added by agents)
  - excerpt: (to be added by agents)
EOF

    success "Repository context updated"
    success "Language detected: $language"
    info "Review and refine repo_context.md if needed"
    echo ""
}

# Refine command - run multi-agent refinement pipeline
cmd_security_agent() {
    local ticket_id=""

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -*)
                error "Unknown option: $1"
                ;;
            *)
                if [ -z "$ticket_id" ]; then
                    ticket_id="$1"
                    shift
                else
                    error "Unexpected argument: $1"
                fi
                ;;
        esac
    done

    # Validate required arguments
    [ -z "$ticket_id" ] && error "Ticket ID is required"

    # Ensure ticket directory exists
    local ticket_dir="$TICKETS_DIR/$ticket_id"
    [ ! -d "$ticket_dir" ] && error "Ticket not found: $ticket_id"

    local spec_file="$ticket_dir/spec.md"
    local repo_context_file="$ticket_dir/repo_context.md"

    # Validate required files exist
    [ ! -f "$spec_file" ] && error "spec.md not found for $ticket_id"
    [ ! -f "$repo_context_file" ] && error "repo_context.md not found for $ticket_id"

    info "Running Security & Compliance Agent on $ticket_id..."
    echo ""

    # Run the security compliance agent
    if security_compliance_agent "$ticket_dir"; then
        success "Security & Compliance Agent completed successfully"
        echo ""
        info "Updated: $spec_file"
        echo ""
        info "Next steps:"
        echo "  - Review the 'Security and compliance' section in spec.md"
        echo "  - Run: vault67 refine $ticket_id (to run full pipeline)"
    else
        error "Security & Compliance Agent failed"
    fi
}

# Fallback test strategy generator when external agent unavailable
test_strategy_agent() {
    local ticket_dir="$1"
    local spec_file="$ticket_dir/spec.md"
    local repo_context_file="$ticket_dir/repo_context.md"
    local rules_file="$SCRIPT_DIR/agents/test_strategy.md"

    [ ! -f "$spec_file" ] && error "spec.md not found: $spec_file"

    info "Running Test Strategy Agent (rule-based + LLM)..."

    # Check if test strategy already has substantive content (layers defined)
    local existing_test=$(sed -n '/^## Test strategy/,/^## [^#]/p' "$spec_file" | sed '1d;$d')
    if echo "$existing_test" | grep -qE "^- (Unit|Integration|E2E|e2e) tests:"; then
        success "Test strategy already contains test layers - preserving"
        return 0
    fi

    local gherkin=$(sed -n '/## Acceptance Criteria (Gherkin)/,/^## /p' "$spec_file" | sed '$d')
    local scenario_count=$(echo "$gherkin" | grep -c "Scenario:" || true)

    # --- Step 1: Determine golden commands (existing logic) ---
    local build_cmd="N/A (no compilation required)"
    local test_cmd="Manual verification against acceptance criteria"
    local detected_marker=""

    if [ -f "$rules_file" ]; then
        local file_default_build=$(sed -n '/^## Default commands$/,/^## /p' "$rules_file" | grep "^- Golden build command:" | sed 's/^- Golden build command:[[:space:]]*//')
        local file_default_test=$(sed -n '/^## Default commands$/,/^## /p' "$rules_file" | grep "^- Golden test command:" | sed 's/^- Golden test command:[[:space:]]*//')
        [ -n "$file_default_build" ] && build_cmd="$file_default_build"
        [ -n "$file_default_test" ] && test_cmd="$file_default_test"

        # Check language-specific rules against repo context
        local repo_path=$(grep "^- Path/URL:" "$repo_context_file" 2>/dev/null | sed 's/^- Path\/URL:[[:space:]]*//')
        if [ -n "$repo_path" ] && [ -d "$repo_path" ] 2>/dev/null; then
            local in_lang=false
            while IFS= read -r line; do
                case "$line" in
                    "## Language-specific commands"*) in_lang=true; continue ;;
                    "## "*) [ "$in_lang" = true ] && break; continue ;;
                    "#"*|"") continue ;;
                esac
                if [ "$in_lang" = true ] && [[ "$line" == -\ * ]]; then
                    local marker=$(echo "$line" | sed 's/^- //' | cut -d: -f1 | tr -d ' ')
                    local cmds=$(echo "$line" | cut -d: -f2-)
                    if [ -f "$repo_path/$marker" ]; then
                        build_cmd=$(echo "$cmds" | cut -d'|' -f1 | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
                        test_cmd=$(echo "$cmds" | cut -d'|' -f2 | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
                        detected_marker="$marker"
                        break
                    fi
                fi
            done < "$rules_file"
        fi
    fi

    # --- Step 2: Determine test layers from Gherkin keywords ---
    local layers_unit=""
    local layers_integration=""
    local layers_e2e=""
    local layer_details_unit=""
    local layer_details_integration=""
    local layer_details_e2e=""

    if [ -f "$rules_file" ]; then
        local in_layers=false
        while IFS= read -r line; do
            case "$line" in
                "## Test layers"*) in_layers=true; continue ;;
                "## "*) [ "$in_layers" = true ] && break; continue ;;
                "#"*|"") continue ;;
            esac
            if [ "$in_layers" = true ] && [[ "$line" == -\ * ]]; then
                # Parse: keywords: layer | framework | description
                local keywords=$(echo "$line" | sed 's/^- //' | cut -d: -f1 | tr -d ' ')
                local rest=$(echo "$line" | cut -d: -f2-)
                local layer=$(echo "$rest" | cut -d'|' -f1 | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
                local framework=$(echo "$rest" | cut -d'|' -f2 | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
                local description=$(echo "$rest" | cut -d'|' -f3 | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')

                # Check if any keyword matches the Gherkin content
                local IFS_SAVE="$IFS"
                IFS=','
                for kw in $keywords; do
                    if echo "$gherkin" | grep -qi "$kw"; then
                        case "$layer" in
                            unit)
                                if [ -z "$layers_unit" ]; then
                                    layers_unit="$framework"
                                    layer_details_unit="$description"
                                elif ! echo "$layer_details_unit" | grep -q "$description"; then
                                    layer_details_unit="$layer_details_unit, $(echo "$description" | awk '{print tolower(substr($0,1,1)) substr($0,2)}')"
                                fi
                                ;;
                            integration)
                                if [ -z "$layers_integration" ]; then
                                    layers_integration="$framework"
                                    layer_details_integration="$description"
                                elif ! echo "$layer_details_integration" | grep -q "$description"; then
                                    layer_details_integration="$layer_details_integration, $(echo "$description" | awk '{print tolower(substr($0,1,1)) substr($0,2)}')"
                                fi
                                ;;
                            e2e)
                                if [ -z "$layers_e2e" ]; then
                                    layers_e2e="$framework"
                                    layer_details_e2e="$description"
                                elif ! echo "$layer_details_e2e" | grep -q "$description"; then
                                    layer_details_e2e="$layer_details_e2e, $(echo "$description" | awk '{print tolower(substr($0,1,1)) substr($0,2)}')"
                                fi
                                ;;
                        esac
                        break  # One keyword match per rule is enough
                    fi
                done
                IFS="$IFS_SAVE"
            fi
        done < "$rules_file"
    fi

    # Apply default test layer if nothing matched
    if [ -z "$layers_unit" ] && [ -z "$layers_integration" ] && [ -z "$layers_e2e" ]; then
        if [ -f "$rules_file" ]; then
            local default_line=$(sed -n '/^## Default test layer$/,/^## /p' "$rules_file" | grep -v '^##' | grep -v '^$' | head -1)
            if [ -n "$default_line" ]; then
                layers_unit=$(echo "$default_line" | cut -d'|' -f1 | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
                layer_details_unit=$(echo "$default_line" | cut -d'|' -f3 | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
            fi
        fi
        [ -z "$layers_unit" ] && layers_unit="pytest/jest" && layer_details_unit="Test business logic functions in isolation"
    fi

    # --- Step 3: Resolve frameworks per detected language ---
    local lint_cmd=""
    local typecheck_cmd=""
    if [ -f "$rules_file" ] && [ -n "$detected_marker" ]; then
        local fw_line=$(sed -n '/^## Framework mapping$/,/^## /p' "$rules_file" | grep "^- ${detected_marker}:")
        if [ -n "$fw_line" ]; then
            local fw_rest=$(echo "$fw_line" | cut -d: -f2-)
            local fw_unit=$(echo "$fw_rest" | cut -d'|' -f1 | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
            local fw_integ=$(echo "$fw_rest" | cut -d'|' -f2 | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
            local fw_e2e=$(echo "$fw_rest" | cut -d'|' -f3 | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
            lint_cmd=$(echo "$fw_rest" | cut -d'|' -f4 | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
            typecheck_cmd=$(echo "$fw_rest" | cut -d'|' -f5 | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')

            # Override generic framework names with language-specific ones
            [ -n "$layers_unit" ] && [ -n "$fw_unit" ] && layers_unit="$fw_unit"
            [ -n "$layers_integration" ] && [ -n "$fw_integ" ] && layers_integration="$fw_integ"
            [ -n "$layers_e2e" ] && [ -n "$fw_e2e" ] && [ "$fw_e2e" != "-" ] && layers_e2e="$fw_e2e"
        fi
    fi

    # --- Step 4: Read quality gates from rules ---
    local quality_gates=""
    if [ -f "$rules_file" ]; then
        local in_qg=false
        while IFS= read -r line; do
            case "$line" in
                "## Quality gates"*) in_qg=true; continue ;;
                "## "*) [ "$in_qg" = true ] && break; continue ;;
                "#"*|"") continue ;;
            esac
            if [ "$in_qg" = true ] && [[ "$line" == -\ * ]]; then
                local gate_text=$(echo "$line" | sed 's/^- //')
                # Substitute lint/typecheck commands if we know them
                if [ -n "$lint_cmd" ]; then
                    gate_text=$(echo "$gate_text" | sed "s/\`eslint\`\/\`ruff\`\/\`clippy\` depending on stack/\`${lint_cmd}\`/")
                fi
                if [ -n "$typecheck_cmd" ] && [ "$typecheck_cmd" != "(built-in)" ]; then
                    gate_text=$(echo "$gate_text" | sed "s/\`tsc --noEmit\`\/\`mypy\`\/\`pyright\` if applicable/\`${typecheck_cmd}\`/")
                fi
                quality_gates="${quality_gates}- ${gate_text}\n"
            fi
        done < "$rules_file"
    fi
    # Fallback quality gates
    if [ -z "$quality_gates" ]; then
        quality_gates="- All tests pass (zero failures)\n- No hardcoded secrets or credentials in test code\n"
    fi

    # --- Step 5: Build rule-based output ---
    local test_layers_section=""
    [ -n "$layers_unit" ] && test_layers_section="${test_layers_section}- Unit tests: ${layers_unit} — ${layer_details_unit}\n"
    [ -n "$layers_integration" ] && test_layers_section="${test_layers_section}- Integration tests: ${layers_integration} — ${layer_details_integration}\n"
    [ -n "$layers_e2e" ] && test_layers_section="${test_layers_section}- E2E tests: ${layers_e2e} — ${layer_details_e2e}\n"

    # Build scenario list for LLM prompt / fallback table
    local scenario_names=""
    local scenario_table=""
    if [ "$scenario_count" -gt 0 ] 2>/dev/null; then
        scenario_names=$(echo "$gherkin" | grep "Scenario:" | sed 's/.*Scenario:[[:space:]]*//')
    fi

    # --- Step 6: LLM enhancement (optional) ---
    local llm_scenario_mapping=""
    if [ "$scenario_count" -gt 0 ] 2>/dev/null && [ -n "$scenario_names" ]; then
        local prompt="You are a test strategy advisor. Given these Gherkin scenarios and the detected test layers, map each scenario to a specific test layer, framework, and suggested file location.

GHERKIN SCENARIOS:
${gherkin}

DETECTED TEST LAYERS:
$(echo -e "$test_layers_section")

REPO LANGUAGE MARKER: ${detected_marker:-unknown}

INSTRUCTIONS:
For each Scenario, output EXACTLY one line in this markdown table format:
| Scenario name | layer | framework | suggested file path |

Rules:
- layer must be one of: unit, integration, e2e
- framework should match the detected frameworks above
- file path should follow standard conventions (tests/ or e2e/ directory)
- Output ONLY the table rows, no header, no fences, no explanation
- One line per scenario, starting with |"

        local llm_response
        llm_response=$(ollama_generate "$prompt" 2>/dev/null) || true

        if [ -n "$llm_response" ]; then
            # Extract only data rows (skip header/separator rows the LLM echoes back)
            llm_scenario_mapping=$(echo "$llm_response" | grep '^|' | \
                grep -vi '| *Scenario name\b\|| *Scenario *|\|| *layer *|\|^| *-' | head -20)
        fi
    fi

    # Build scenario mapping (LLM or fallback)
    if [ -n "$llm_scenario_mapping" ]; then
        scenario_table="| Scenario | Layer | Framework | Suggested location |\n|----------|-------|-----------|-------------------|\n${llm_scenario_mapping}"
    elif [ "$scenario_count" -gt 0 ] 2>/dev/null && [ -n "$scenario_names" ]; then
        # Fallback: simple mapping based on detected layers
        scenario_table="| Scenario | Layer | Framework | Suggested location |\n|----------|-------|-----------|-------------------|\n"
        while IFS= read -r scenario_name; do
            [ -z "$scenario_name" ] && continue
            # Guess layer from scenario name keywords
            local s_layer="unit" s_fw="${layers_unit:-pytest/jest}" s_loc="tests/"
            if echo "$scenario_name" | grep -qiE "UI|page|form|button|dashboard|click|display|render"; then
                s_layer="e2e"; s_fw="${layers_e2e:-Playwright}"; s_loc="e2e/"
            elif echo "$scenario_name" | grep -qiE "api|endpoint|route|login|auth|database|query|email|webhook|file|upload|download"; then
                s_layer="integration"; s_fw="${layers_integration:-httpx/supertest}"; s_loc="tests/integration/"
            fi
            local safe_name=$(echo "$scenario_name" | tr '[:upper:]' '[:lower:]' | sed 's/[^a-z0-9]/_/g' | sed 's/__*/_/g' | sed 's/^_//;s/_$//')
            scenario_table="${scenario_table}| ${scenario_name} | ${s_layer} | ${s_fw} | ${s_loc}${safe_name}.test | \n"
        done <<< "$scenario_names"
    fi

    # --- Step 7: Assemble and write to spec ---
    local temp_test=$(mktemp)
    {
        echo "## Test strategy"
        echo "- Golden build command: ${build_cmd}"
        echo "- Golden test command: ${test_cmd}"
        echo ""
        echo "### Test layers"
        echo -e "$test_layers_section"
        if [ -n "$scenario_table" ]; then
            echo "### Scenario mapping"
            echo -e "$scenario_table"
        fi
        echo "### Quality gates"
        echo -e "$quality_gates"
    } > "$temp_test"

    awk '
        /^## Test strategy/ {
            system("cat '"$temp_test"'")
            in_section = 1
            next
        }
        /^## / && in_section {
            in_section = 0
        }
        !in_section { print }
    ' "$spec_file" > "$spec_file.tmp" && mv "$spec_file.tmp" "$spec_file"

    rm -f "$temp_test"

    if [ -n "$llm_scenario_mapping" ]; then
        success "Test strategy generated (rule-based + LLM scenario mapping)"
    else
        success "Test strategy generated (rule-based)"
    fi
    return 0
}

cmd_refine() {
    local issue_number=""

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -*)
                error "Unknown option: $1"
                ;;
            *)
                if [ -z "$issue_number" ]; then
                    issue_number="$1"
                    shift
                else
                    error "Unexpected argument: $1"
                fi
                ;;
        esac
    done

    # Validate required arguments
    [ -z "$issue_number" ] && error "Issue number is required"

    # Ensure API config is loaded
    require_api_config

    info "Fetching issue #${issue_number} from Forgejo..."
    
    # Get issue from API
    local api_response=$(api_get_issue "$issue_number")
    local issue_body=$(echo "$api_response" | extract_issue_body)
    local current_labels=$(echo "$api_response" | extract_issue_labels)
    
    info "Current labels: ${current_labels}"
    
    # Check if issue is in valid state for refinement
    if ! echo "$current_labels" | grep -qE "state:(NEW|NEEDS_INFO|REFINING)"; then
        error "Issue must be in state:NEW, state:NEEDS_INFO, or state:REFINING (current: ${current_labels})"
    fi

    # Update label to REFINING
    info "Starting refinement pipeline..."
    api_replace_labels "$issue_number" "state:REFINING" > /dev/null
    success "State updated to REFINING"
    echo ""

    # Create temporary working directory
    local temp_dir=$(mktemp -d)
    local spec_file="$temp_dir/spec.md"
    local repo_context_file="$temp_dir/repo_context.md"
    local questions_file="$temp_dir/questions.md"

    # Write issue body to temporary spec file
    echo "$issue_body" > "$spec_file"

    # Create minimal repo_context.md (extract from spec metadata if present)
    local repo_path=$(echo "$issue_body" | grep "^- Repo:" | sed 's/^- Repo: `\(.*\)`/\1/')
    local base_ref=$(echo "$issue_body" | grep "^- Base ref:" | sed 's/^- Base ref: `\(.*\)`/\1/')
    
    cat > "$repo_context_file" <<REPO_EOF
# Repo Context

## Repo
- Path/URL: ${repo_path}
- Base ref: ${base_ref}
- Language/runtime:
- Main components/modules:

## How to build (golden command)
- Command(s):

## How to test (golden command)
- Command(s):
REPO_EOF

    # Initialize questions file
    cat > "$questions_file" <<QUES_EOF
# Questions (Human in the loop)

## Blocking questions

## Notes
-
QUES_EOF

    # Run agents in pipeline order (using temporary files)
    # Agents will update the spec file in place
    # BA Translator runs FIRST so downstream agents analyze real Gherkin

    # Run BA Translator Agent (LLM-powered Gherkin generation)
    info "Running BA Translator Agent..."
    if ba_translator_agent "$temp_dir"; then
        success "BA Translator Agent completed"
    else
        warn "BA Translator Agent could not generate Gherkin (continuing with existing)"
    fi
    echo ""

    # Run Architecture Compliance Agent
    info "Running Architecture Compliance Agent..."
    if architecture_compliance_agent "$temp_dir"; then
        success "Architecture Compliance Agent completed"
    else
        warn "Architecture Compliance Agent reported issues (continuing to Judge)"
    fi
    echo ""

    # Run Security & Compliance Agent
    info "Running Security & Compliance Agent..."
    if security_compliance_agent "$temp_dir"; then
        success "Security & Compliance Agent completed"
    else
        warn "Security & Compliance Agent reported issues (continuing to Judge)"
    fi
    echo ""

    # Run Test Strategy Agent (rule-based + LLM enhancement)
    test_strategy_agent "$temp_dir"
    echo ""

    # Read updated spec back
    local updated_spec=$(cat "$spec_file")

    # Run Judge Agent (capture exit code without triggering set -e)
    local judge_result=0
    judge_agent "$spec_file" "$questions_file" || judge_result=$?

    local timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
    local new_label=""
    local comment_body=""

    if [ $judge_result -eq 2 ]; then
        # NEEDS_INFO - blocking questions exist
        new_label="state:NEEDS_INFO"
        
        local questions_content=$(cat "$questions_file")
        comment_body="## ⚠️ Blocking Questions

The refinement pipeline identified blocking questions that need answers before proceeding:

${questions_content}

**Next steps:**
1. Answer the blocking questions by editing this comment or adding a new comment
2. Run: \`vault67 answer ${issue_number}\`"

        warn "Ticket requires human input"

    elif [ $judge_result -eq 0 ]; then
        # READY_TO_IMPLEMENT - all criteria met
        new_label="state:READY_TO_IMPLEMENT"

        # Generate promptpack
        local promptpack_content="# Implementation Prompt Pack

Generated: ${timestamp}

## Spec
${updated_spec}

## Instructions
1. Implement changes according to the specification above
2. Follow all constraints and allowed/forbidden paths
3. Create tests as defined in test strategy
4. Create a PR against base ref
5. Include a brief PR description mapping changes to scenarios"

        comment_body="## ✅ Ready for Implementation

The specification has passed all refinement criteria.

<details>
<summary>Promptpack (click to expand)</summary>

${promptpack_content}

</details>

**Next steps:**
Run: \`vault67 implement ${issue_number}\`"

        success "Ticket is ready for implementation!"

    else
        # Not ready - criteria not met but no blocking questions
        new_label="state:REFINING"
        comment_body="## ⚠️ Refinement Incomplete

The specification does not yet meet all Definition of Ready criteria. Please review and complete the missing sections.

**Next steps:**
1. Edit the issue body to fill in missing sections
2. Run: \`vault67 refine ${issue_number}\`"

        warn "Criteria not met - ticket requires more work"
    fi

    # Update issue body with refined spec
    info "Updating issue body..."
    api_update_issue "$issue_number" "$updated_spec" > /dev/null
    success "Issue body updated"

    # Update label
    info "Updating state to ${new_label}..."
    api_replace_labels "$issue_number" "$new_label" > /dev/null
    success "State updated to ${new_label}"

    # Add comment with status and next steps
    if [ -n "$comment_body" ]; then
        info "Adding status comment..."
        api_add_comment "$issue_number" "$comment_body" > /dev/null
        success "Comment added"
    fi

    # Clean up temp directory
    rm -rf "$temp_dir"

    echo ""
    success "Refinement complete"
    success "New state: ${new_label}"
    success "View issue: https://git.logikfabriken.se/${FORGEJO_REPO}/issues/${issue_number}"
}


# Answer command - validate questions are answered and re-enable refinement
cmd_answer() {
    local issue_number=""

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            -*)
                error "Unknown option: $1"
                ;;
            *)
                if [ -z "$issue_number" ]; then
                    issue_number="$1"
                    shift
                else
                    error "Unexpected argument: $1"
                fi
                ;;
        esac
    done

    # Validate required arguments
    [ -z "$issue_number" ] && error "Issue number is required"

    # Ensure API config is loaded
    require_api_config

    info "Fetching issue #${issue_number} from Forgejo..."
    
    # Get issue from API
    local api_response=$(api_get_issue "$issue_number")
    local current_labels=$(echo "$api_response" | extract_issue_labels)
    
    info "Current labels: ${current_labels}"

    # Check if issue is in NEEDS_INFO state
    if ! echo "$current_labels" | grep -q "state:NEEDS_INFO"; then
        warn "Issue is not in state:NEEDS_INFO (current: ${current_labels})"
        warn "This command is only needed when blocking questions exist"
        return 0
    fi

    # Get comments to check if answers were provided
    info "Checking for answers in comments..."
    local comments=$(api_get_comments "$issue_number")
    
    # Simple check: if there are comments after the blocking questions comment, assume answers provided
    # (In a more sophisticated version, we could parse the comments to verify answers)
    
    info "Answers appear to have been provided in comments"

    # Update label to REFINING
    info "Updating state to REFINING..."
    api_replace_labels "$issue_number" "state:REFINING" > /dev/null
    success "State updated to REFINING - ready for refinement pipeline"

    # Add a status comment
    local comment_body="## ✅ Answers Provided

State changed from NEEDS_INFO to REFINING. The refinement pipeline can now re-run.

**Next steps:**
Run: \`vault67 refine ${issue_number}\`"

    api_add_comment "$issue_number" "$comment_body" > /dev/null

    echo ""
    success "Issue ready for re-refinement"
    success "View issue: https://git.logikfabriken.se/${FORGEJO_REPO}/issues/${issue_number}"
    echo ""
    info "Next steps:"
    echo "  1. Run: vault67 refine ${issue_number}"
}


# Implement command - hand off to Gas Town
cmd_implement() {
    local issue_number=""
    local executor="gastown"

    # Parse arguments
    while [[ $# -gt 0 ]]; do
        case $1 in
            --executor)
                executor="$2"
                shift 2
                ;;
            -*)
                error "Unknown option: $1"
                ;;
            *)
                if [ -z "$issue_number" ]; then
                    issue_number="$1"
                    shift
                else
                    error "Unexpected argument: $1"
                fi
                ;;
        esac
    done

    # Validate required arguments
    [ -z "$issue_number" ] && error "Issue number is required"

    # Ensure API config is loaded
    require_api_config

    info "Fetching issue #${issue_number} from Forgejo..."
    
    # Get issue from API
    local api_response=$(api_get_issue "$issue_number")
    local issue_title=$(echo "$api_response" | python3 -c "import sys, json; print(json.load(sys.stdin)['title'])")
    local current_labels=$(echo "$api_response" | extract_issue_labels)
    
    info "Current labels: ${current_labels}"

    # Check if issue is in READY_TO_IMPLEMENT state
    if ! echo "$current_labels" | grep -q "state:READY_TO_IMPLEMENT"; then
        error "Issue must be in state:READY_TO_IMPLEMENT (current: ${current_labels})"
    fi

    # Update label to IMPLEMENTING
    info "Updating state to IMPLEMENTING..."
    api_replace_labels "$issue_number" "state:IMPLEMENTING" > /dev/null
    success "State updated to IMPLEMENTING"

    # Get comments to find the promptpack
    info "Fetching promptpack from comments..."
    local comments=$(api_get_comments "$issue_number")
    
    # Extract promptpack from the "Ready for Implementation" comment
    # (In a real implementation, we'd parse the JSON to find the specific comment)
    local promptpack_url="https://git.logikfabriken.se/${FORGEJO_REPO}/issues/${issue_number}#issuecomment"

    echo ""
    success "Issue ready for implementation!"
    success "Issue #${issue_number}: ${issue_title}"
    success "Promptpack available in issue comments"
    echo ""
    
    # Add implementation status comment
    local timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
    local comment_body="## 🚧 Implementation Started

Timestamp: ${timestamp}
Executor: ${executor}
Status: Handed off to Gas Town

**Instructions for implementer:**
1. Review the promptpack in the comments above
2. Implement according to the specification
3. Create tests as defined in test strategy
4. Create a PR and link it to this issue
5. Update issue state when complete

**Gas Town handoff:**
\`\`\`bash
gt sling --assign <polecat> --work 'Implement: ${issue_title} (#${issue_number})'
\`\`\`

Provide the polecat with:
- Issue URL: https://git.logikfabriken.se/${FORGEJO_REPO}/issues/${issue_number}
- Promptpack: See comment above"

    api_add_comment "$issue_number" "$comment_body" > /dev/null
    success "Implementation comment added"

    echo ""
    info "Next steps:"
    echo "  1. Review promptpack: https://git.logikfabriken.se/${FORGEJO_REPO}/issues/${issue_number}"
    echo "  2. Hand off to Gas Town using:"
    echo "     gt sling --assign <polecat> --work 'Implement: ${issue_title} (#${issue_number})'"
    echo ""
    echo "  3. Provide the polecat with the issue URL and promptpack"
    echo "  4. After completion, update issue state to DONE"
}


cmd_delete() {
    local issue_number=""
    local force=false

    while [ $# -gt 0 ]; do
        case "$1" in
            --force|-f) force=true; shift ;;
            [0-9]*) issue_number="$1"; shift ;;
            *) error "Usage: vault67 delete <issue-number> [--force]" ;;
        esac
    done
    [ -z "$issue_number" ] && error "Usage: vault67 delete <issue-number> [--force]"
    require_api_config

    # Fetch issue to confirm it exists
    local api_response
    api_response=$(api_request "GET" "/repos/${FORGEJO_REPO}/issues/${issue_number}" 2>/dev/null) || {
        error "Issue #${issue_number} not found"
    }

    local issue_title=$(echo "$api_response" | python3 -c "import sys,json; print(json.load(sys.stdin)['title'])")
    local issue_state=$(echo "$api_response" | python3 -c "import sys,json; print(json.load(sys.stdin)['state'])")
    local labels=$(echo "$api_response" | extract_issue_labels)

    info "Issue #${issue_number}: ${issue_title}"
    info "State: ${labels} (${issue_state})"

    # Confirm unless --force
    if [ "$force" != true ]; then
        echo -en "${YELLOW}Are you sure you want to close and mark as deleted? [y/N] ${NC}"
        read -r confirm
        if [[ ! "$confirm" =~ ^[Yy] ]]; then
            info "Cancelled"
            return 0
        fi
    fi

    # Close the issue and add a deletion comment
    local json_data=$(python3 -c "import json; print(json.dumps({'state': 'closed'}))")
    api_request "PATCH" "/repos/${FORGEJO_REPO}/issues/${issue_number}" "$json_data" > /dev/null

    # Remove state labels and add DONE
    api_replace_labels "$issue_number" "state:DONE" > /dev/null

    # Add deletion comment
    api_add_comment "$issue_number" "Issue closed via \`vault67 delete ${issue_number}\`" > /dev/null

    success "Issue #${issue_number} closed and marked as DONE"
}

# Translate a project-created issue into a full spec without asking questions
# Project issues have enough context from the feature title + project prompt
_translate_project_issue() {
    local issue_number="$1"
    local issue_title="$2"
    local raw_text="$3"
    local today="$4"

    # Extract project name and prompt from body
    local project_name=$(echo "$raw_text" | grep "Created from project:" | sed 's/.*\*\*\(.*\)\*\*/\1/')
    local project_prompt=$(echo "$raw_text" | sed -n 's/^> //p' | head -1)

    local spec_body="# Specification

**Metadata:**
- Repo: \`https://git.logikfabriken.se/${FORGEJO_REPO}.git\`
- Base ref: \`main\`
- Created: ${today}

## Context
Part of project: **${project_name}**

Project scope: ${project_prompt}

This feature: ${issue_title}

## Goal
${issue_title}

## Scope
### In scope
- ${issue_title}

### Out of scope
- Unrelated project features are handled by separate issues
- Infrastructure, deployment, and CI/CD pipeline changes
- Performance optimization beyond functional requirements

## Requirements (Raw, BA input)
- ${issue_title}
- Part of project: ${project_name}

## Acceptance Criteria (Gherkin)
\`\`\`gherkin
Feature: ${issue_title}

  Scenario: ${issue_title} is implemented
    Given the project ${project_name} is being developed
    When ${issue_title} is complete
    Then the feature works as specified
\`\`\`

## Architecture alignment
- Relevant modules:
- Constraints:
- Allowed paths:
- Forbidden paths:

## Security and compliance
- Data classification:
- AuthN/AuthZ:
- Logging/Audit:
- PII/Secrets:
- Security constraints:

## Test strategy
- Golden build command:
- Golden test command:
- Scenario to test mapping:

## Engineering principles and DoD additions
-

## Open questions
-

## Definition of Done
- PR created with changes scoped correctly
- Tests added/updated
- All required checks green
- Acceptance criteria satisfied
- Documentation updated (if needed)

## Definition of Ready
- [ ] Scope in/out defined
- [ ] Gherkin scenarios are present and testable
- [ ] Architecture alignment reviewed and constraints captured
- [ ] Security/compliance reviewed and constraints captured
- [ ] Test strategy defined for each scenario
- [ ] Repo golden commands known or explicitly blocked
- [ ] Allowed/forbidden paths set
- [ ] No blocking questions remain"

    # Update issue body with structured spec
    api_update_issue "$issue_number" "$spec_body" > /dev/null
    success "Issue #${issue_number} translated to structured spec (project mode)"
    return 0
}

# BA Translator: convert plain text issue body into structured spec
cmd_translate() {
    local issue_number=""
    while [ $# -gt 0 ]; do
        case "$1" in
            [0-9]*) issue_number="$1"; shift ;;
            *) error "Usage: vault67 translate <issue-number>" ;;
        esac
    done
    [ -z "$issue_number" ] && error "Usage: vault67 translate <issue-number>"
    require_api_config

    info "Fetching issue #${issue_number}..."
    local api_response=$(api_get_issue "$issue_number")
    local issue_body=$(echo "$api_response" | extract_issue_body)
    local issue_title=$(echo "$api_response" | python3 -c "import sys,json; print(json.load(sys.stdin)['title'])")

    # Check if already has spec structure
    if echo "$issue_body" | grep -q "^# Specification"; then
        info "Issue #${issue_number} already has spec structure - skipping translate"
        return 0
    fi

    info "Translating plain text to structured spec..."

    local raw_text="$issue_body"
    local today=$(date -u +"%Y-%m-%d")

    # Detect project-created issues — these have enough context to skip questions
    if echo "$raw_text" | grep -q "Created from project:"; then
        info "Detected project-created issue — generating spec from project context"
        _translate_project_issue "$issue_number" "$issue_title" "$raw_text" "$today"
        return $?
    fi

    # Analyze text quality - identify what's missing
    local questions=""
    local question_count=0

    # Check: can we identify concrete features/actions?
    local action_lines=$(echo "$raw_text" | grep -ciE "create|add|implement|build|delete|update|list|show|display|send|import|export|connect|integrate" || true)
    if [ "$action_lines" -lt 1 ]; then
        question_count=$((question_count + 1))
        questions="${questions}${question_count}) What specific actions/commands should this feature support?\n   Answer: \n\n"
    fi

    # Check: do we know who the user is?
    local has_actor=$(echo "$raw_text" | grep -ciE "user|admin|developer|agent|daemon|system|cli" || true)
    if [ "$has_actor" -lt 1 ]; then
        question_count=$((question_count + 1))
        questions="${questions}${question_count}) Who is the primary user of this feature (human user, CLI, agent, daemon)?\n   Answer: \n\n"
    fi

    # Check: are there clear success criteria?
    local has_outcome=$(echo "$raw_text" | grep -ciE "should|must|will|expect|result|output|return|display|produce" || true)
    if [ "$has_outcome" -lt 1 ]; then
        question_count=$((question_count + 1))
        questions="${questions}${question_count}) What does success look like? What should the output/result be?\n   Answer: \n\n"
    fi

    # Check: are there external dependencies mentioned without specifics?
    if echo "$raw_text" | grep -qiE "api|integration|connect|external|service"; then
        local has_api_detail=$(echo "$raw_text" | grep -ciE "endpoint|url|route|method|GET|POST|PUT|DELETE" || true)
        if [ "$has_api_detail" -lt 1 ]; then
            question_count=$((question_count + 1))
            questions="${questions}${question_count}) Which specific API endpoints or external services are involved?\n   Answer: \n\n"
        fi
    fi

    # Check: is this a big feature that needs breakdown?
    local word_count=$(echo "$raw_text" | wc -w | tr -d ' ')
    local sentence_count=$(echo "$raw_text" | grep -c '\.' || true)
    if echo "$raw_text" | grep -qiE "break.?down|features|tasks|multiple|several|project|phases"; then
        question_count=$((question_count + 1))
        questions="${questions}${question_count}) This sounds like a large feature. Can you list the individual sub-features/tasks?\n   Answer: \n\n"
    fi

    # Check: what's out of scope?
    question_count=$((question_count + 1))
    questions="${questions}${question_count}) What is explicitly out of scope for this feature?\n   Answer: \n\n"

    # Extract goal from text
    local goal_lines=$(echo "$raw_text" | grep -iE "want|should|must|will|need" | head -3)
    [ -z "$goal_lines" ] && goal_lines="$issue_title"

    # Extract scope hints
    local in_scope=""
    local scope_lines=$(echo "$raw_text" | grep -iE "include|support|add|implement|create|build|break|use" | head -5)
    if [ -n "$scope_lines" ]; then
        while IFS= read -r line; do
            [ -z "$line" ] && continue
            line=$(echo "$line" | sed 's/^[[:space:]]*//')
            [[ "$line" != -* ]] && line="- $line"
            in_scope="${in_scope}${line}\n"
        done <<< "$scope_lines"
    fi
    [ -z "$in_scope" ] && in_scope="- $issue_title"

    # Build spec with questions embedded
    local spec_body="# Specification

**Metadata:**
- Repo: \`https://git.logikfabriken.se/${FORGEJO_REPO}.git\`
- Base ref: \`main\`
- Created: ${today}

## Context
${raw_text}

## Goal
${goal_lines}

## Scope
### In scope
$(echo -e "$in_scope")

### Out of scope
- Awaiting clarification (see blocking questions)

## Requirements (Raw, BA input)
${raw_text}

## Acceptance Criteria (Gherkin)
\`\`\`gherkin
Feature: ${issue_title}

  Scenario: TODO - needs clarification before writing scenarios
    Given prerequisites are defined
    When actions are specified
    Then expected outcomes are documented
\`\`\`

## Architecture alignment
- Relevant modules:
- Constraints:
- Allowed paths:
- Forbidden paths:

## Security and compliance
- Data classification:

## Test strategy
- Golden build command:
- Golden test command:
"

    # Update the issue body
    api_update_issue "$issue_number" "$spec_body" > /dev/null

    # Post questions as a comment and set state to NEEDS_INFO
    if [ "$question_count" -gt 0 ]; then
        local questions_comment="## Blocking Questions

The following questions need answers before this ticket can be refined:

$(echo -e "$questions")
**How to proceed:**
1. Reply to this comment with answers
2. Edit the issue body to update the spec
3. Change the label back to \`state:NEW\` to re-trigger refinement"

        api_add_comment "$issue_number" "$questions_comment" > /dev/null
        api_replace_labels "$issue_number" "state:NEEDS_INFO" > /dev/null
        warn "Issue #${issue_number} needs clarification - ${question_count} question(s) posted"
        return 2  # Signal NEEDS_INFO to caller
    fi

    success "Issue #${issue_number} translated to structured spec"
}

# ============================================================================
# Project Command - Milestones + auto-breakdown
# ============================================================================

cmd_project() {
    require_api_config

    local subcmd="${1:-}"
    [ -z "$subcmd" ] && error "Usage: vault67 project <create|list|status> [args]"
    shift

    case "$subcmd" in
        create) _project_create "$@" ;;
        list)   _project_list "$@" ;;
        status) _project_status "$@" ;;
        run)    _project_run "$@" ;;
        *)      error "Unknown project subcommand: $subcmd" ;;
    esac
}

_project_create() {
    local name=""
    local description=""

    # Parse: vault67 project create "Name" "description"
    # or: vault67 project create --name "Name" --description "description"
    while [ $# -gt 0 ]; do
        case "$1" in
            --name) name="$2"; shift 2 ;;
            --description|--desc) description="$2"; shift 2 ;;
            *)
                if [ -z "$name" ]; then
                    name="$1"; shift
                elif [ -z "$description" ]; then
                    description="$1"; shift
                else
                    error "Unexpected argument: $1"
                fi
                ;;
        esac
    done

    [ -z "$name" ] && error "Usage: vault67 project create <name> <description>"
    [ -z "$description" ] && error "Usage: vault67 project create <name> <description>"

    info "Creating project: $name"

    # Step 1: Create milestone
    local milestone_json=$(python3 -c "
import json, sys
print(json.dumps({'title': sys.argv[1], 'description': sys.argv[2]}))
" "$name" "$description")

    local milestone_response=$(api_request "POST" "/repos/${FORGEJO_REPO}/milestones" "$milestone_json")
    local milestone_id=$(echo "$milestone_response" | python3 -c "import sys,json; print(json.load(sys.stdin)['id'])")
    success "Created milestone #${milestone_id}: $name"

    # Step 2: Break down description into features/tasks
    info "Breaking down project prompt into features..."

    local features=""
    features=$(_extract_features "$description")

    if [ -z "$features" ]; then
        warn "Could not extract features from description. Creating single issue."
        features="$name"
    fi

    # Step 3: Create an issue for each feature, assign to milestone, label state:NEW
    local issue_count=0
    local -a created_issues=()    # Track (issue_number feature_text) pairs
    local -a feature_texts=()     # Track feature texts for dependency analysis

    while IFS= read -r feature; do
        [ -z "$feature" ] && continue
        # Clean leading whitespace/bullets
        feature=$(echo "$feature" | sed 's/^[[:space:]]*[-*0-9.)]*[[:space:]]*//')
        [ -z "$feature" ] && continue

        info "  Creating issue: $feature"

        local issue_json=$(python3 -c "
import json, sys
print(json.dumps({
    'title': sys.argv[1],
    'body': sys.argv[2],
    'milestone': int(sys.argv[3])
}))
" "$feature" "Created from project: **${name}**

Project prompt:
> ${description}

This is feature/task extracted from the project breakdown." "$milestone_id")

        local issue_response=$(api_request "POST" "/repos/${FORGEJO_REPO}/issues" "$issue_json")
        local issue_num=$(echo "$issue_response" | python3 -c "import sys,json; print(json.load(sys.stdin)['number'])")

        # Add state:NEW label so daemon picks it up
        api_replace_labels "$issue_num" "state:NEW" > /dev/null 2>&1 || true
        success "  Issue #${issue_num}: $feature [state:NEW]"

        created_issues+=("$issue_num")
        feature_texts+=("$feature")
        issue_count=$((issue_count + 1))
    done <<< "$features"

    # Step 4: Detect and set dependencies between created issues
    if [ ${#created_issues[@]} -gt 1 ]; then
        info "Analyzing dependencies between features..."
        local dep_json=$(_detect_feature_dependencies "${feature_texts[@]}")

        # Parse dependency pairs and set them via API
        local dep_count=0
        while IFS= read -r dep_line; do
            [ -z "$dep_line" ] && continue
            local dep_from=$(echo "$dep_line" | cut -d' ' -f1)  # feature index (0-based)
            local dep_on=$(echo "$dep_line" | cut -d' ' -f2)    # depends-on index (0-based)

            local from_issue="${created_issues[$dep_from]}"
            local on_issue="${created_issues[$dep_on]}"

            info "  Setting dependency: #${from_issue} depends on #${on_issue}"
            api_add_dependency "$from_issue" "$on_issue" > /dev/null 2>&1 || {
                warn "  Failed to set dependency #${from_issue} → #${on_issue}"
                continue
            }
            dep_count=$((dep_count + 1))
        done <<< "$dep_json"

        if [ $dep_count -gt 0 ]; then
            success "Set ${dep_count} dependency relationship(s)"
        else
            info "No dependencies detected between features"
        fi
    fi

    echo ""
    success "Project '$name' created with ${issue_count} issue(s)"
    success "Milestone: #${milestone_id}"
    info "The daemon will auto-refine each issue (respecting dependency order)"
    info "View: https://git.logikfabriken.se/${FORGEJO_REPO}/milestone/${milestone_id}"
}

# Extract features from a project description
# Looks for numbered items, bullet points, or "Feature N:" patterns
_extract_features() {
    local text="$1"

    python3 -c "
import sys, re

text = sys.argv[1]
features = []

# Try: 'Feature N: ...' pattern - split on it
if re.search(r'Feature\s*\d+[:.]\s*', text, re.IGNORECASE):
    parts = re.split(r'Feature\s*\d+[:.]\s*', text, flags=re.IGNORECASE)
    for p in parts:
        p = p.strip().rstrip('.')
        if p:
            features.append(p)
# Try: numbered list '1. ...' or '1) ...'
elif re.search(r'^\s*\d+[.)]\s+', text, re.MULTILINE):
    num_matches = re.findall(r'^\s*\d+[.)]\s*(.+)', text, re.MULTILINE)
    for f in num_matches:
        features.append(f.strip())
# Try: bullet points '- ...' or '* ...'
elif re.search(r'^\s*[-*]\s+', text, re.MULTILINE):
    bullet_matches = re.findall(r'^\s*[-*]\s+(.+)', text, re.MULTILINE)
    for f in bullet_matches:
        features.append(f.strip())
# Try: sentence splitting on periods
else:
    sentences = [s.strip() for s in text.split('.') if len(s.strip()) > 10]
    if len(sentences) > 1:
        for s in sentences:
            features.append(s.strip())
    else:
        features.append(text.strip())

for f in features:
    print(f)
" "$text"
}

# Detect dependencies between features using keyword analysis
# Args: feature_text_1 feature_text_2 ... (all feature texts as separate args)
# Output: lines of "FROM_INDEX DEPENDS_ON_INDEX" (0-based)
_detect_feature_dependencies() {
    local -a texts=("$@")

    python3 -c "
import sys, re

texts = sys.argv[1:]
n = len(texts)
if n < 2:
    sys.exit(0)

deps = []

# Strategy 1: Explicit dependency signals
# If feature N mentions concepts from feature M (M < N), N depends on M
dep_keywords = re.compile(r'\b(after|requires|depends on|once .+ is done|builds on|using|based on|extends|on top of)\b', re.IGNORECASE)

for i in range(n):
    text_i = texts[i].lower()

    # Check for explicit dependency signals
    if dep_keywords.search(text_i):
        # Look for references to earlier features
        for j in range(i):
            # Extract key nouns from feature j (words 4+ chars, not common words)
            stop_words = {'with', 'that', 'this', 'from', 'will', 'have', 'been', 'should', 'could', 'would', 'into', 'each', 'more', 'also', 'than', 'when', 'then', 'some', 'only'}
            words_j = set(w for w in re.findall(r'[a-z]{4,}', texts[j].lower()) if w not in stop_words)
            words_i = set(w for w in re.findall(r'[a-z]{4,}', text_i) if w not in stop_words)
            overlap = words_j & words_i
            if len(overlap) >= 1:
                deps.append((i, j))
                break  # Only depend on the nearest match

    # Strategy 2: Sequential numbering implies sequential dependency
    # If features came from 'Feature 1: ... Feature 2: ... Feature 3: ...'
    # they are implicitly sequential
    if i > 0 and (i, i-1) not in deps:
        # Check if text has sequential signal (using/with concepts from prior)
        stop_words = {'with', 'that', 'this', 'from', 'will', 'have', 'been', 'should', 'could', 'would', 'into', 'each', 'more', 'also', 'than', 'when', 'then', 'some', 'only'}
        words_prev = set(w for w in re.findall(r'[a-z]{4,}', texts[i-1].lower()) if w not in stop_words)
        words_curr = set(w for w in re.findall(r'[a-z]{4,}', text_i) if w not in stop_words)
        overlap = words_prev & words_curr
        # If there's meaningful overlap with the previous feature, add dependency
        if len(overlap) >= 2:
            deps.append((i, i-1))

# Deduplicate
seen = set()
for d in deps:
    if d not in seen:
        seen.add(d)
        print(f'{d[0]} {d[1]}')
" "${texts[@]}"
}

# Run a project end-to-end: create → process all issues in dependency order
_project_run() {
    local name=""
    local description=""

    while [ $# -gt 0 ]; do
        case "$1" in
            --name) name="$2"; shift 2 ;;
            --description|--desc) description="$2"; shift 2 ;;
            *)
                if [ -z "$name" ]; then
                    name="$1"; shift
                elif [ -z "$description" ]; then
                    description="$1"; shift
                else
                    error "Unexpected argument: $1"
                fi
                ;;
        esac
    done

    [ -z "$name" ] && error "Usage: vault67 project run <name> <description>"
    [ -z "$description" ] && error "Usage: vault67 project run <name> <description>"

    # Step 1: Create the project (milestone + issues + deps)
    _project_create "$name" "$description"

    echo ""
    echo "=========================================="
    info "Starting inline pipeline processing..."
    echo "=========================================="
    echo ""

    # Step 2: Get the milestone we just created (latest one with this name)
    local milestones_response=$(api_request "GET" "/repos/${FORGEJO_REPO}/milestones?state=open&limit=10")
    local milestone_id=$(echo "$milestones_response" | python3 -c "
import sys, json
milestones = json.load(sys.stdin)
for m in sorted(milestones, key=lambda x: x['id'], reverse=True):
    if m['title'] == sys.argv[1]:
        print(m['id'])
        break
" "$name" 2>/dev/null)

    [ -z "$milestone_id" ] && error "Could not find milestone for project: $name"

    # Step 3: Get all issues in this milestone
    local issues_response=$(api_request "GET" "/repos/${FORGEJO_REPO}/issues?milestones=${milestone_id}&state=open&type=issues&limit=50")
    local all_issue_nums=$(echo "$issues_response" | python3 -c "
import sys, json
issues = json.load(sys.stdin)
for i in sorted(issues, key=lambda x: x['number']):
    print(i['number'])
" 2>/dev/null)

    local total_issues=$(echo "$all_issue_nums" | wc -l | tr -d ' ')
    local processed=0
    local max_rounds=$((total_issues * 2))  # Safety limit
    local round=0

    info "Processing ${total_issues} issues in dependency order..."
    echo ""

    # Step 4: Process in rounds — each round processes unblocked issues
    while [ $processed -lt "$total_issues" ] && [ $round -lt $max_rounds ]; do
        round=$((round + 1))
        local progress_made=false

        for issue_num in $all_issue_nums; do
            # Check if already processed (not state:NEW)
            local issue_data=$(api_get_issue "$issue_num" 2>/dev/null) || continue
            local labels=$(echo "$issue_data" | extract_issue_labels)

            # Skip if not in a processable state
            if ! echo "$labels" | grep -q "state:NEW"; then
                continue
            fi

            # Check dependencies
            local blockers
            blockers=$(api_check_deps_resolved "$issue_num" 2>/dev/null) || true
            if [ -n "$blockers" ]; then
                local blocker_list=$(echo "$blockers" | tr '\n' ',' | sed 's/,$//')
                info "  #${issue_num}: blocked by #${blocker_list} — skipping this round"
                continue
            fi

            # Process this issue
            echo "-------------------------------------------"
            info "Processing issue #${issue_num} (round ${round})"
            echo "-------------------------------------------"

            # Translate
            "$SCRIPT_DIR/vault67" translate "$issue_num" 2>&1 | while IFS= read -r line; do echo "  $line"; done || true

            # Re-check state after translate (may have changed to NEEDS_INFO)
            local post_translate=$(api_get_issue "$issue_num" 2>/dev/null)
            local post_labels=$(echo "$post_translate" | extract_issue_labels)

            if echo "$post_labels" | grep -q "state:NEEDS_INFO"; then
                warn "  #${issue_num}: needs info after translate — skipping refine"
                processed=$((processed + 1))
                progress_made=true
                continue
            fi

            # Ensure still in NEW state for refine
            if echo "$post_labels" | grep -qE "state:(NEW|REFINING)"; then
                "$SCRIPT_DIR/vault67" refine "$issue_num" 2>&1 | while IFS= read -r line; do echo "  $line"; done || true
            fi

            processed=$((processed + 1))
            progress_made=true
            echo ""
        done

        # If no progress was made this round, we're stuck
        if [ "$progress_made" = false ]; then
            warn "No progress in round ${round} — remaining issues may have circular or external dependencies"
            break
        fi
    done

    echo ""
    echo "=========================================="
    info "Pipeline complete. Processed ${processed}/${total_issues} issues."
    echo "=========================================="

    # Show final status
    _project_status "$milestone_id"
}

_project_list() {
    local state="open"
    while [ $# -gt 0 ]; do
        case "$1" in
            --all) state="all"; shift ;;
            --closed) state="closed"; shift ;;
            *) shift ;;
        esac
    done

    local response=$(api_request "GET" "/repos/${FORGEJO_REPO}/milestones?state=${state}&limit=50")

    python3 -c "
import sys, json
milestones = json.load(sys.stdin)
if not milestones:
    print('No projects found.')
    sys.exit(0)

hdr = '{:<6} {:<8} {:<10} {}'.format('ID', 'State', 'Issues', 'Title')
print(hdr)
print('-' * 60)
for m in milestones:
    mid = m['id']
    state = m['state']
    total = m.get('open_issues', 0) + m.get('closed_issues', 0)
    open_i = m.get('open_issues', 0)
    title = m['title']
    ratio = '{}/{}'.format(open_i, total)
    print('{:<6} {:<8} {:<10} {}'.format(mid, state, ratio, title))
" <<< "$response"
}

_project_status() {
    local milestone_id="${1:-}"
    [ -z "$milestone_id" ] && error "Usage: vault67 project status <milestone-id>"

    # Get milestone info
    local milestone=$(api_request "GET" "/repos/${FORGEJO_REPO}/milestones/${milestone_id}")
    local milestone_title=$(echo "$milestone" | python3 -c "import sys,json; print(json.load(sys.stdin)['title'])")

    echo ""
    echo "Project: $milestone_title"
    echo "=========================================="

    # Get all issues in this milestone (milestones= is the correct Forgejo filter)
    local issues=$(api_request "GET" "/repos/${FORGEJO_REPO}/issues?milestones=${milestone_id}&state=all&type=issues&limit=50")

    # Build dependency map: issue_number -> [dep_numbers that are still open]
    local issue_numbers
    issue_numbers=$(echo "$issues" | python3 -c "
import sys, json
for i in json.load(sys.stdin):
    print(i['number'])
" 2>/dev/null)

    local dep_map="{}"
    if [ -n "$issue_numbers" ]; then
        local dep_entries=""
        for inum in $issue_numbers; do
            local blockers
            blockers=$(api_check_deps_resolved "$inum" 2>/dev/null) || true
            if [ -n "$blockers" ]; then
                local blocker_json=$(echo "$blockers" | python3 -c "import sys,json; print(json.dumps([int(l) for l in sys.stdin.read().strip().split()]))" 2>/dev/null)
                dep_entries="${dep_entries}\"${inum}\": ${blocker_json}, "
            fi
        done
        if [ -n "$dep_entries" ]; then
            dep_map="{${dep_entries%%, }}"
        fi
    fi

    python3 -c "
import sys, json

data = json.loads(sys.argv[1])
dep_map = json.loads(sys.argv[2])

issues = data
if not issues:
    print('No issues in this project.')
    sys.exit(0)

# Group by state label
groups = {}
for issue in issues:
    labels = [l['name'] for l in issue.get('labels', [])]
    state_label = next((l for l in labels if l.startswith('state:')), 'no-state')
    state = state_label.replace('state:', '')
    if state not in groups:
        groups[state] = []
    groups[state].append(issue)

# Display in pipeline order
order = ['NEW', 'REFINING', 'NEEDS_INFO', 'READY_TO_IMPLEMENT', 'IMPLEMENTING', 'DONE', 'no-state']
total = len(issues)
done = len(groups.get('DONE', []))

print(f'Progress: {done}/{total} done')
print()

for state in order:
    if state not in groups:
        continue
    items = groups[state]
    print(f'--- {state} ({len(items)}) ---')
    for issue in items:
        num = issue['number']
        title = issue['title'][:60]
        closed = ' (closed)' if issue['state'] == 'closed' else ''
        blockers = dep_map.get(str(num), [])
        blocked_str = ''
        if blockers:
            refs = ', '.join(f'#{b}' for b in blockers)
            blocked_str = f' [blocked by {refs}]'
        print(f'  #{num} {title}{closed}{blocked_str}')
    print()
" "$issues" "$dep_map"
}

# ============================================================================
# Deps Command - Manual dependency management
# ============================================================================

cmd_deps() {
    require_api_config

    local subcmd="${1:-}"
    [ -z "$subcmd" ] && error "Usage: vault67 deps <add|remove|show|check> [args]"
    shift

    case "$subcmd" in
        add)    _deps_add "$@" ;;
        remove) _deps_remove "$@" ;;
        show)   _deps_show "$@" ;;
        check)  _deps_check "$@" || exit $? ;;
        *)      error "Unknown deps subcommand: $subcmd (use add|remove|show|check)" ;;
    esac
}

# vault67 deps add 13 12  →  issue #13 depends on #12
_deps_add() {
    local issue="${1:-}"
    local depends_on="${2:-}"
    [ -z "$issue" ] || [ -z "$depends_on" ] && error "Usage: vault67 deps add <issue> <depends-on>"

    info "Setting dependency: #${issue} depends on #${depends_on}"
    api_add_dependency "$issue" "$depends_on" > /dev/null
    success "Dependency set: #${issue} → #${depends_on}"
}

# vault67 deps remove 13 12  →  remove dependency
_deps_remove() {
    local issue="${1:-}"
    local depends_on="${2:-}"
    [ -z "$issue" ] || [ -z "$depends_on" ] && error "Usage: vault67 deps remove <issue> <depends-on>"

    info "Removing dependency: #${issue} no longer depends on #${depends_on}"
    api_remove_dependency "$issue" "$depends_on" > /dev/null
    success "Dependency removed: #${issue} ✗ #${depends_on}"
}

# vault67 deps show 13  →  show what #13 depends on and what it blocks
_deps_show() {
    local issue="${1:-}"
    [ -z "$issue" ] && error "Usage: vault67 deps show <issue>"

    echo ""
    echo "Issue #${issue} dependencies:"
    echo ""

    # What does this issue depend on?
    local deps_response
    deps_response=$(api_get_dependencies "$issue" 2>/dev/null) || deps_response="[]"

    echo "  Depends on:"
    python3 -c "
import sys, json
deps = json.load(sys.stdin)
if not deps:
    print('    (none)')
else:
    for d in deps:
        state = '✓ closed' if d.get('state') == 'closed' else '○ open'
        print(f'    #{d[\"number\"]} {d[\"title\"][:50]} [{state}]')
" <<< "$deps_response"

    echo ""

    # What does this issue block?
    local blocks_response
    blocks_response=$(api_get_blocks "$issue" 2>/dev/null) || blocks_response="[]"

    echo "  Blocks:"
    python3 -c "
import sys, json
blocks = json.load(sys.stdin)
if not blocks:
    print('    (none)')
else:
    for b in blocks:
        state = '✓ closed' if b.get('state') == 'closed' else '○ open'
        print(f'    #{b[\"number\"]} {b[\"title\"][:50]} [{state}]')
" <<< "$blocks_response"
    echo ""
}

# vault67 deps check 13  →  exit 0 if all deps resolved, exit 1 + print blockers if blocked
# Used internally by vault67-watch daemon
_deps_check() {
    local issue="${1:-}"
    [ -z "$issue" ] && error "Usage: vault67 deps check <issue>"

    local result=0
    api_check_deps_resolved "$issue" || result=$?
    return $result
}

# Done command - mark issue as DONE, close it, report unblocked downstream
cmd_done() {
    local issue_number=""

    while [[ $# -gt 0 ]]; do
        case $1 in
            [0-9]*) issue_number="$1"; shift ;;
            *) error "Usage: vault67 done <issue-number>" ;;
        esac
    done

    [ -z "$issue_number" ] && error "Usage: vault67 done <issue-number>"
    require_api_config

    info "Marking issue #${issue_number} as DONE..."

    # Set state:DONE label
    api_replace_labels "$issue_number" "state:DONE" > /dev/null

    # Close the issue on Forgejo
    local json_data=$(python3 -c "import json; print(json.dumps({'state': 'closed'}))")
    api_request "PATCH" "/repos/${FORGEJO_REPO}/issues/${issue_number}" "$json_data" > /dev/null

    # Add completion comment
    local timestamp=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
    api_add_comment "$issue_number" "Issue completed and closed at ${timestamp} via \`vault67 done\`" > /dev/null

    success "Issue #${issue_number} marked as DONE and closed"

    # Check what this issue was blocking
    local blocks_response
    blocks_response=$(api_get_blocks "$issue_number" 2>/dev/null) || blocks_response="[]"

    local unblocked
    unblocked=$(python3 -c "
import sys, json
blocks = json.load(sys.stdin)
if not blocks:
    sys.exit(0)
for b in blocks:
    if b.get('state') != 'closed':
        print(b['number'])
" <<< "$blocks_response" 2>/dev/null) || true

    if [ -n "$unblocked" ]; then
        echo ""
        info "Downstream issues potentially unblocked:"
        for blocked_num in $unblocked; do
            # Check if ALL deps for this blocked issue are now resolved
            local still_blocked
            still_blocked=$(api_check_deps_resolved "$blocked_num" 2>/dev/null) || true
            if [ -z "$still_blocked" ]; then
                success "  #${blocked_num} — all dependencies resolved, ready for processing"
            else
                local remaining=$(echo "$still_blocked" | tr '\n' ',' | sed 's/,$//')
                info "  #${blocked_num} — still blocked by #${remaining}"
            fi
        done
    fi
}

cmd_list() {
    require_api_config

    local state="open"
    while [ $# -gt 0 ]; do
        case "$1" in
            --all) state="all"; shift ;;
            *) error "Unknown option: $1" ;;
        esac
    done

    local endpoint="/repos/${FORGEJO_REPO}/issues?state=${state}&type=issues&limit=50"
    local response
    response=$(api_request "GET" "$endpoint")

    python3 -c "
import sys, json

issues = json.loads(sys.stdin.read())
if not issues:
    print('No open issues found' if '${state}' == 'open' else 'No issues found')
    sys.exit(0)

# Color codes per state
colors = {
    'state:NEW': '\033[0;34m',
    'state:REFINING': '\033[1;33m',
    'state:NEEDS_INFO': '\033[0;35m',
    'state:READY_TO_IMPLEMENT': '\033[0;32m',
    'state:IMPLEMENTING': '\033[1;33m',
    'state:DONE': '\033[0;90m',
}
NC = '\033[0m'

for issue in sorted(issues, key=lambda i: i['number']):
    labels = [l['name'] for l in issue.get('labels', [])]
    state_label = next((l for l in labels if l.startswith('state:')), '')
    tag = state_label.replace('state:', '') if state_label else '-'
    c = colors.get(state_label, NC)
    closed = ' [closed]' if issue.get('state') == 'closed' else ''
    print(f'  #{issue[\"number\"]:3d}  {c}{tag:22s}{NC}  {issue[\"title\"]}{closed}')
" <<< "$response"
}

cmd_status() {
    require_api_config

    local issue_number=""

    # Parse args
    if [ $# -gt 0 ] && [[ "$1" =~ ^[0-9]+$ ]]; then
        issue_number="$1"
    fi

    if [ -n "$issue_number" ]; then
        # Detail mode: show single issue
        local response
        response=$(api_request "GET" "/repos/${FORGEJO_REPO}/issues/${issue_number}" 2>/dev/null) || {
            error "Issue #${issue_number} not found"
        }

        python3 -c "
import sys, json
issue = json.loads(sys.stdin.read())
title = issue['title']
number = issue['number']
state = issue.get('state', 'open')
labels = [l['name'] for l in issue.get('labels', [])]
state_label = next((l for l in labels if l.startswith('state:')), 'state:UNKNOWN')
created = issue['created_at'][:10]
updated = issue['updated_at'][:10]
comments = issue.get('comments', 0)

# Color codes
colors = {
    'state:NEW': '\033[0;34m',
    'state:REFINING': '\033[1;33m',
    'state:NEEDS_INFO': '\033[0;35m',
    'state:READY_TO_IMPLEMENT': '\033[0;32m',
    'state:IMPLEMENTING': '\033[1;33m',
    'state:DONE': '\033[0;90m',
}
NC = '\033[0m'
BOLD = '\033[1m'
c = colors.get(state_label, NC)

print(f'{BOLD}Issue #{number}{NC}: {title}')
print(f'  State:    {c}{state_label}{NC}')
print(f'  Created:  {created}')
print(f'  Updated:  {updated}')
print(f'  Comments: {comments}')
if len(labels) > 1:
    other = [l for l in labels if not l.startswith('state:')]
    if other:
        olabels = ', '.join(other)
        print(f'  Labels:   {olabels}')
" <<< "$response"

    else
        # Overview mode: show pipeline summary
        info "Fetching issues from Forgejo..."

        local response
        response=$(api_request "GET" "/repos/${FORGEJO_REPO}/issues?state=open&type=issues&limit=50")

        python3 -c "
import sys, json

issues = json.loads(sys.stdin.read())
if not issues:
    print('No issues found')
    sys.exit(0)

# Count by state
counts = {}
state_order = ['state:NEW', 'state:REFINING', 'state:NEEDS_INFO', 'state:READY_TO_IMPLEMENT', 'state:IMPLEMENTING', 'state:DONE']
for s in state_order:
    counts[s] = []

for issue in issues:
    labels = [l['name'] for l in issue.get('labels', [])]
    state_label = next((l for l in labels if l.startswith('state:')), 'state:UNKNOWN')
    if state_label not in counts:
        counts[state_label] = []
    counts[state_label].append(issue)

# Color codes
colors = {
    'state:NEW': '\033[0;34m',
    'state:REFINING': '\033[1;33m',
    'state:NEEDS_INFO': '\033[0;35m',
    'state:READY_TO_IMPLEMENT': '\033[0;32m',
    'state:IMPLEMENTING': '\033[1;33m',
    'state:DONE': '\033[0;90m',
}
NC = '\033[0m'
BOLD = '\033[1m'

print(f'{BOLD}Pipeline Overview{NC} ({len(issues)} open issues)')
print()

for state in state_order:
    items = counts.get(state, [])
    if not items:
        continue
    c = colors.get(state, NC)
    label = state.replace('state:', '')
    print(f'  {c}{label}{NC} ({len(items)})')
    for issue in items:
        print(f'    #{issue[\"number\"]:3d}  {issue[\"title\"]}')
    print()
" <<< "$response"
    fi
}

# Main command dispatcher
main() {
    # Load configuration
    load_config

    if [ $# -eq 0 ]; then
        cat <<'HELPEOF'
vault67 - CLI for multi-agent ticket refinement (Forgejo API)

Configuration:
  Set FORGEJO_TOKEN, FORGEJO_API, and FORGEJO_REPO in:
  - .vault67.conf file (see .vault67.conf.example)
  - Environment variables

Usage:
  vault67 create --title "<title>" --repo "<path_or_url>" [--base-ref "main"]
  vault67 refine <issue-number>
  vault67 answer <issue-number>
  vault67 implement <issue-number> [--executor gastown]
  vault67 done <issue-number>
  vault67 delete <issue-number> [--force]
  vault67 translate <issue-number>
  vault67 list [--all]
  vault67 status [issue-number]
  vault67 project create <name> <description>
  vault67 project run <name> <description>
  vault67 project list [--all]
  vault67 project status <milestone-id>
  vault67 deps add <issue> <depends-on>
  vault67 deps remove <issue> <depends-on>
  vault67 deps show <issue>

Commands:
  create          Create a new issue on Forgejo with spec template
  refine          Run multi-agent refinement pipeline on an issue
  answer          Mark blocking questions as answered (HITL)
  implement       Hand off issue to Gas Town for implementation
  done            Mark issue as DONE, close it, report unblocked issues
  delete          Close an issue (with confirmation)
  translate       Convert plain text issue to structured spec
  list            List issues with state and title
  status          Show pipeline overview or single issue details
  project         Manage projects (create, run, list, status)
  deps            Manage issue dependencies (add/remove/show/check)

Examples:
  vault67 create --title "Add rate limiting" --repo "/repos/my-service"
  vault67 refine 42
  vault67 answer 42
  vault67 implement 42
  vault67 list
  vault67 list --all
  vault67 status 2
  vault67 project create "Auth System" "1. Add login page 2. Add JWT tokens 3. Add role-based access"
  vault67 project list
  vault67 project status 1
  vault67 deps add 13 12     # Issue #13 depends on #12
  vault67 deps show 13       # Show deps and blocks for #13
HELPEOF
        exit 0
    fi

    local cmd=$1
    shift

    case $cmd in
        create)
            cmd_create "$@"
            ;;
        refine)
            cmd_refine "$@"
            ;;
        security_agent)
            cmd_security_agent "$@"
            ;;
        answer)
            cmd_answer "$@"
            ;;
        implement)
            cmd_implement "$@"
            ;;
        done)
            cmd_done "$@"
            ;;
        delete)
            cmd_delete "$@"
            ;;
        translate)
            cmd_translate "$@"
            ;;
        list)
            cmd_list "$@"
            ;;
        status)
            cmd_status "$@"
            ;;
        project)
            cmd_project "$@"
            ;;
        deps)
            cmd_deps "$@"
            ;;
        *)
            error "Unknown command: $cmd"
            ;;
    esac
}

main "$@"

